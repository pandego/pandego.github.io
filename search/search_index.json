{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About Me","text":""},{"location":"#hi-im-miguel-i-build-ai-systems-and-document-the-journey","title":"Hi, I'm Miguel \ud83d\udc4b\ud83c\udffc I build AI systems and document the journey","text":""},{"location":"#a-10-years-journey-turning-ai-theory-into-production-reality","title":"A 10+ years journey turning AI theory into production reality","text":"<ul> <li>Wondering how AI solves YOUR specific business problems?</li> <li>Tired of AI projects that never make it past the PowerPoint?</li> <li>Overwhelmed sorting real AI solutions from the noise?</li> <li>Worried about hallucinations, privacy, and vendor lock-in?</li> <li>Need a clear roadmap from AI pilot to production?</li> <li>Ready to be in the 5%<sup>1</sup> whose AI solutions actually bring ROI?</li> </ul> <p>Book Intro Call </p>"},{"location":"#about-me","title":"About me","text":"<p>My mission is simple \u2192 Protect startups and teams from buying AI they can't use.</p> <p>After a decade building AI systems across medtech, automotive, and enterprise sectors, I've seen too many companies burned by consultancies selling PowerPoints instead of working code. Technical teams get sidelined, projects fail, and AI becomes another expensive disappointment. I founded Datavengers to change that equation.</p> <p>My path here wasn't typical. I started in research (PhD in genomics, postdoc in gene therapy) before recognizing that AI would reshape every industry. So I pivoted! Earned my Master's in Data Science &amp; Artificial Intelligence and dove into industry, where I ship production-grade AI systems that actually deliver ROI. My academic foundation gave me the rigor, but it's the years of shipping production code that taught me what actually works. From patent analysis systems processing thousands of documents to computer vision catching defects on production lines, I've learned what works beyond the proof-of-concept stage. This unique background means I can translate between data scientists writing the code, executives signing the checks, and users living with the results.</p> <p>Today, through Datavengers and my educational content, I help companies implement AI that sticks and developers avoid the mistakes I've made. Because the real challenge isn't understanding AI - it's making it work reliably, at scale, with real data and real constraints. That's where I come in.</p>"},{"location":"#what-people-say-about-working-with-me","title":"What people say about working with me","text":"<ul> <li> <p> Ludovic Gardy</p> <p>Founder at Sotis Advanced Insights</p> <p>\"Miguel stands out for his ability to quickly grasp complex topics, understand critical challenges, and identify key issues. His expertise in AI and MLOps enables him to build robust pipelines and contribute effectively to end-to-end solutions, always delivering clean, efficient execution with a proactive mindset. Importantly, he excels in an industrial environment by skillfully managing project constraints and task coordination while developing comprehensive roadmaps with clearly defined scopes.\"</p> </li> <li> <p> Nicolas Baillot d'Estivaux</p> <p>Chief Data Officer at bioM\u00e9rieux</p> <p>\"Miguel has a very strong technical background, he's able to lead important projects, to make the accurate technical choices and always learns new things and try to implement them in the profesional context. His support was highly valuable for the data team and for the company.\"</p> </li> <li> <p> Mehdi Elion</p> <p>Senior Data Scientist at Mirakle</p> <p>\"Miguel has definitely shown the best qualities one could dream of in an AI Tech Lead. He's kind, passionate, supportive, encourages you to grow both professionally and technically. He's also one of the most reliable techies I've worked with: Miguel is the go-to guy whether it is for pure data science, hardware setup, solution deployment or even networking issues. It has been an honor to work with him, and I strongly recommend to have him in your team if you get the opportunity to do so.\"</p> </li> <li> <p> Antonio Jacinto</p> <p>Plant Manager at Forvia</p> <p>\"Miguel's expertise in machine learning and deep learning, particularly in computer vision and its application to industrial cases has been pivotal in our success. His programming skills are exceptional, and his ability to navigate complex technical challenges and to propose innovative solutions significantly contributed to the success of our initiatives. His contributions have left a lasting impact.\"</p> </li> </ul>"},{"location":"#frequently-asked-questions","title":"Frequently asked questions","text":"How quickly can you start working on my project? <p>I can typically begin new projects within 1-2 weeks of contract signing. For urgent matters, I maintain some flexibility for rapid response situations and can potentially start sooner - just let me know your timeline during our initial consultation.</p> Do you require a minimum project size or commitment? <p>While I can accommodate projects of any size, I find that engagements of at least 20 hours per week allow for meaningful impact. This gives us enough time to understand your data, implement solutions, and deliver actionable results. We can start with a small pilot project to ensure we're a good fit.</p> What industries do you have experience in? <p>I've successfully delivered projects across biotech, healthcare, manufacturing, and automotive sectors. While I specialize in data science &amp; AI fundamentals that apply across sectors, I particularly excel in projects involving the implementation of AI solutions such as RAG and LLM integration for enterprise applications.</p> How do you handle data security and confidentiality? <p>I take data security extremely seriously. I sign comprehensive NDAs before starting any project, use enterprise-grade encryption for all data transfers, and follow industry best practices for data handling. I can also work within your existing security infrastructure and policies.</p> What's your pricing structure? <p>I offer both project-based and retainer pricing models. Project fees are based on scope, complexity, and value delivered rather than hours worked. For ongoing support, I offer flexible retainer packages. Let's discuss your specific needs during our consultation to determine the most cost-effective approach.</p> How do you communicate progress and results? <p>I maintain clear communication through weekly progress updates and regular check-in meetings. You'll receive detailed documentation of all analyses, findings, and recommendations. For ongoing projects, I provide interactive dashboards and reports that allow you to track progress and results in real-time.</p> <ul> <li> <p> Let's have a virtual coffee together!</p> <p>Want to see if we're a match? Let's have a chat and find out. Schedule a free 30-minute strategy session to discuss your AI challenges and explore how we can work together.</p> <p>Book Intro Call </p> </li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2023/03/05/red-wine-and-dockerized-mlflow-a-love-story-between-devops-and-data-science/","title":"Red Wine and Dockerized MLflow: A Love Story Between DevOps and Data Science","text":"<p>Are you tired of messy machine learning pipelines? Do you struggle to keep track of your experiments and their results? Fear not, because MLflow and Docker Compose are here to save the day!</p> <p>Condensed mini\u2011blog from my piece on self-hosting an MLflow instance.</p> <p>Read full article on Medium.</p>"},{"location":"blog/2023/03/05/red-wine-and-dockerized-mlflow-a-love-story-between-devops-and-data-science/#why-this-stack-works","title":"Why this stack works","text":"<p>If you\u2019re wrangling ML experiments, MLflow is your control tower. Pair it with PostgreSQL for the tracking backend and MinIO (S3\u2011compatible) for artifact storage, then wrap the whole thing in Docker Compose so it\u2019s reproducible and easy to boot up anywhere. Result: clean experiment tracking, model registry, and artifact management without yak\u2011shaving.</p> <p>Ingredients</p> <ul> <li>MLflow: experiment tracking, model registry, model serving</li> <li>Postgres: durable metadata (experiments, runs, metrics)</li> <li>MinIO: object store for artifacts/models (S3 API)</li> <li>Docker Compose: one command to run them all</li> </ul>"},{"location":"blog/2023/03/05/red-wine-and-dockerized-mlflow-a-love-story-between-devops-and-data-science/#prereqs","title":"Prereqs","text":"<ul> <li>Ubuntu 20.04 LTS (WSL2 on Windows works too)</li> <li>Miniconda, Docker, Docker Compose</li> <li>Optional but handy: pyenv to keep Python versions isolated</li> </ul> <pre><code># Install pyenv (Ubuntu)\nsudo apt-get update -y &amp;&amp; \\\n  sudo apt-get install -y make build-essential libssl-dev zlib1g-dev \\\n  libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm \\\n  libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev \\\n  libffi-dev liblzma-dev\ncurl https://pyenv.run | bash\n# Add to ~/.bashrc\nexport PYENV_ROOT=\"$HOME/.pyenv\"\nexport PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init --path)\"\nsource ~/.bashrc\n</code></pre> <p>Create a working env and set S3/MLflow env vars:</p> <pre><code>conda create -n mlflow_env python=3.11 -y\nconda activate mlflow_env\npip install pandas scikit-learn mlflow[extras]\n\nexport AWS_ACCESS_KEY_ID=minio\nexport AWS_SECRET_ACCESS_KEY=minio123   # change in .env\nexport MLFLOW_S3_ENDPOINT_URL=http://localhost:9000\n</code></pre>"},{"location":"blog/2023/03/05/red-wine-and-dockerized-mlflow-a-love-story-between-devops-and-data-science/#compose-it-all-together","title":"Compose it all together","text":"<p>Clone the template project:</p> <pre><code>git clone https://github.com/pandego/mlflow-postgres-minio.git\ncd mlflow-postgres-minio\ncp default.env .env   # edit secrets/ports as needed\n</code></pre> <p>Key services (trimmed):</p> <pre><code>services:\n  db:\n    image: postgres:${PG_VERSION}\n    environment:\n      - POSTGRES_USER=${PG_USER}\n      - POSTGRES_PASSWORD=${PG_PASSWORD}\n      - POSTGRES_DATABASE=${PG_DATABASE}\n    healthcheck:\n      test: [\"CMD\", \"pg_isready\", \"-p\", \"${PG_PORT_SERVICE}\", \"-U\", \"${PG_USER}\"]\n\n  s3:\n    image: minio/minio:${MINIO_VERSION}\n    command: server /data --console-address \":9001\" --address \":9000\"\n    environment:\n      - MINIO_ROOT_USER=${MINIO_ROOT_USER}\n      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}\n\n  create_buckets:\n    image: minio/mc:${MINIO_VERSION}\n    depends_on: { s3: { condition: service_healthy } }\n    entrypoint: &gt;\n      /bin/sh -c '\n      sleep 5;\n      mc config host add s3 http://s3:${MINIO_PORT_API} ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD} --api S3v4;\n      mc mb s3/${MLFLOW_BUCKET_NAME} || true;\n      mc policy download s3/${MLFLOW_BUCKET_NAME};\n      mc mb s3/${DATA_REPO_BUCKET_NAME} || true;\n      mc policy download s3/${DATA_REPO_BUCKET_NAME};\n      '\n\n  tracking_server:\n    build: ./mlflow\n    command: &gt;\n      mlflow server \\\n        --backend-store-uri postgresql://${PG_USER}:${PG_PASSWORD}@db:${PG_PORT_SERVICE}/${PG_DATABASE} \\\n        --host 0.0.0.0 \\\n        --port ${MLFLOW_PORT_SERVICE} \\\n        --default-artifact-root s3://mlflow/\n    environment:\n      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}\n      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}\n      - MLFLOW_S3_ENDPOINT_URL=http://s3:${MINIO_PORT_API}\n</code></pre> <p>Bring it up:</p> <pre><code>docker-compose --env-file .env up -d --build\ndocker ps -a   # all services should be healthy\n</code></pre> <p>UIs</p> <ul> <li>MLflow Tracking: http://localhost:5000</li> <li>MinIO Console: http://localhost:9001 (API is :9000)</li> </ul>"},{"location":"blog/2023/03/05/red-wine-and-dockerized-mlflow-a-love-story-between-devops-and-data-science/#train-a-model-elasticnet-on-red-wine","title":"Train a model (ElasticNet on red wine)","text":"<p>Inside <code>./wine_quality_example/</code> you\u2019ll find <code>wine_quality_data.csv</code> and <code>train.py</code>. The script trains a scikit\u2011learn ElasticNet regressor to predict wine quality and logs everything to MLflow.</p> <pre><code># metrics helper\ndef eval_metrics(y_true, y_pred):\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    mae = mean_absolute_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n    return rmse, mae, r2\n\n# point MLflow at your server\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"red-wine-elasticnet\")\n\nwith mlflow.start_run():\n    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n    model.fit(train_x, train_y)\n    preds = model.predict(test_x)\n    rmse, mae, r2 = eval_metrics(test_y, preds)\n\n    mlflow.log_param(\"alpha\", alpha)\n    mlflow.log_param(\"l1_ratio\", l1_ratio)\n    mlflow.log_metric(\"rmse\", rmse)\n    mlflow.log_metric(\"mae\", mae)\n    mlflow.log_metric(\"r2\", r2)\n    mlflow.sklearn.log_model(model, \"model\")\n</code></pre> <p>Run it:</p> <pre><code>cd wine_quality_example\npython train.py\n</code></pre> <p>You should see a new experiment/run in the MLflow UI with params, metrics, and a logged model artifact stored in MinIO.</p>"},{"location":"blog/2023/03/05/red-wine-and-dockerized-mlflow-a-love-story-between-devops-and-data-science/#serve-the-model","title":"Serve the model","text":"<p>Serve any logged run locally with MLflow\u2019s lightweight server:</p> <pre><code>mlflow models serve \\\n  -m s3://mlflow/1/&lt;run_id&gt;/artifacts/model \\\n  -p 1234 --timeout 0\n</code></pre> <p>Smoke\u2011test with <code>curl</code> (JSON <code>dataframe_split</code>):</p> <pre><code>curl -X POST \\\n  -H \"Content-Type: application/json\" \\\n  --data '{\n    \"dataframe_split\": {\n      \"data\": [[7.4,0.7,0,1.9,0.076,11,34,0.9978,3.51,0.56,9.4]],\n      \"columns\": [\n        \"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\n        \"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\n        \"pH\",\"sulphates\",\"alcohol\"\n      ]\n    }\n  }' http://127.0.0.1:1234/invocations\n# \u2192 {\"predictions\": [5.576883967129616]}\n</code></pre>"},{"location":"blog/2023/03/05/red-wine-and-dockerized-mlflow-a-love-story-between-devops-and-data-science/#tips-gotchas","title":"Tips &amp; gotchas","text":"<ul> <li>Keep <code>.env</code> in sync with your conda env variables (keys/ports/bucket names).</li> <li>MinIO exposes API on :9000 and Console on :9001 by default\u2014both must be reachable from the tracking server.</li> <li>Healthchecks in Compose catch misconfig early; don\u2019t disable them.</li> <li>For production, put Postgres/MinIO on persistent volumes you back up.</li> </ul>"},{"location":"blog/2023/03/05/red-wine-and-dockerized-mlflow-a-love-story-between-devops-and-data-science/#wrapup","title":"Wrap\u2011up","text":"<p>MLflow + Postgres + MinIO, dockerized with Compose, gives you a reproducible, portable MLOps base in minutes. Log parameters and metrics, register and serve models, and keep artifacts tidy\u2014all while avoiding snowflake setups. Pour yourself a glass; your experiments are finally organized.</p>"},{"location":"blog/2023/03/05/red-wine-and-dockerized-mlflow-a-love-story-between-devops-and-data-science/#read-the-full-article","title":"\ud83d\udcd6 Read the Full Article","text":"Red Wine, ElasticNet, and Dockerized MLflow with Postgres and MinIO: A Love Story Between DevOps and Data Science <p>A comprehensive guide on setting up MLflow with Docker, using Postgres for tracking and MinIO for artifact storage, demonstrated through a wine quality prediction model.</p>          \ud83d\udcd6 Full article available on Medium"},{"location":"blog/2023/03/05/no-openai-account-no-problem/","title":"No OpenAI Account, No Problem!","text":"<p>If you want the OpenAI developer experience without the OpenAI account (or tokens), Ollama now exposes an OpenAI-style <code>/v1</code> endpoint. That means you can point existing clients and frameworks at your local models and ship.</p> <p>Condensed mini-blog from my piece on crafting your own OpenAI-compatible API with Ollama.</p> <p>Read full article on Medium.</p>"},{"location":"blog/2023/03/05/no-openai-account-no-problem/#why-this-rocks","title":"Why this rocks","text":"<ul> <li>Drop\u2011in compatibility: Keep using the OpenAI SDKs and patterns.</li> <li>Local first: Your data and prompts stay on your machine.</li> <li>Costs: $0 per token, just your hardware.</li> </ul>"},{"location":"blog/2023/03/05/no-openai-account-no-problem/#1-spin-up-ollama-with-docker","title":"1) Spin up Ollama with Docker","text":"<p>CPU only</p> <pre><code>docker run -d -v /data/ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n</code></pre> <p>GPU (NVIDIA)</p> <ol> <li>Install NVIDIA Container Toolkit.</li> <li>Run with GPU access:</li> </ol> <pre><code>docker run -d --gpus=all -v /data/ollama:/root/.ollama --restart always -p 11434:11434 --name ollama ollama/ollama\n</code></pre> <p>Pro tip: Want specific GPUs? Use <code>--gpus \"device=0,1\"</code>.</p> <p>Where are models stored? In this compose, they\u2019ll live on your host at <code>/data/ollama</code>.</p>"},{"location":"blog/2023/03/05/no-openai-account-no-problem/#2-sanitycheck-ollama","title":"2) Sanity\u2011check Ollama","text":"<p>Run a model inside the container:</p> <pre><code>docker exec -it ollama ollama run llama2\n# &gt;&gt;&gt; Send a message (/? for help)\n</code></pre> <p>Hit the OpenAI\u2011style chat completions endpoint:</p> <pre><code>curl http://localhost:11434/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"model\": \"llama2\",\n        \"messages\": [\n          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n          {\"role\": \"user\",   \"content\": \"Hello!\"}\n        ]\n      }'\n</code></pre> <p>If you get a response with <code>choices[0].message.content</code>, you\u2019re golden.</p>"},{"location":"blog/2023/03/05/no-openai-account-no-problem/#3-build-a-local-openai-chatbot","title":"3) Build a local \u201cOpenAI\u201d chatbot","text":"<p>Requirements</p> <pre><code>streamlit&gt;=1.28\nlangchain&gt;=0.0.217\nopenai&gt;=1.2\nduckduckgo-search\nanthropic&gt;=0.3.0\ntrubrics&gt;=1.4.3\nstreamlit-feedback\n</code></pre> <p>Minimal app: <code>Chatbot.py</code></p> <pre><code>from openai import OpenAI\nimport streamlit as st\n\n# This key is required by the SDK but unused by Ollama; leave any string\nOPENAI_API_KEY = \"ollama-baby\"\n\nst.title(\"Chatbot\")\nst.caption(\"A Streamlit chatbot powered by OpenAI API... I mean Ollama!!!\")\n\nif \"messages\" not in st.session_state:\n    st.session_state[\"messages\"] = [\n        {\"role\": \"assistant\", \"content\": \"How can I help you?\"}\n    ]\n\n# Show history\nfor msg in st.session_state[\"messages\"]:\n    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n\n# Input\nprompt = st.chat_input(\"Say something\u2026\")\nif prompt:\n    client = OpenAI(\n        api_key=OPENAI_API_KEY,\n        base_url=\"http://localhost:11434/v1\",  # \u2190 point at Ollama\n    )\n\n    st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": prompt})\n    st.chat_message(\"user\").write(prompt)\n\n    response = client.chat.completions.create(\n        model=\"llama2\",\n        messages=st.session_state[\"messages\"],\n    )\n    msg = response.choices[0].message.content\n\n    st.session_state[\"messages\"].append({\"role\": \"assistant\", \"content\": msg})\n    st.chat_message(\"assistant\").write(msg)\n</code></pre> <p>Run it:</p> <pre><code>streamlit run Chatbot.py\n</code></pre> <p> Still not a standup comedian \ud83d\ude2c</p>"},{"location":"blog/2023/03/05/no-openai-account-no-problem/#notes","title":"Notes","text":"<ul> <li>You can swap <code>llama2</code> for any local model you\u2019ve pulled with Ollama.</li> <li>Keep an eye on VRAM/CPU footprints; bigger models need beefier hardware.</li> <li>LangChain, agents, and retrieval components can ride along since the client looks like OpenAI.</li> </ul>"},{"location":"blog/2023/03/05/no-openai-account-no-problem/#wrapup","title":"Wrap\u2011up","text":"<p>With Ollama\u2019s <code>/v1</code> endpoint, you can prototype and ship OpenAI\u2011compatible apps locally. The DX you know, the privacy you want, and zero token anxiety.</p>"},{"location":"blog/2023/03/05/no-openai-account-no-problem/#read-the-full-article","title":"\ud83d\udcd6 Read the Full Article","text":"No OpenAI Account, No Problem! Crafting Your Own OpenAI API with Ollama \ud83e\udd99 <p>Explore alternatives to OpenAI's services and learn how to leverage open-source models for your AI projects.</p>          \ud83d\udcd6 Full article available on Medium"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/","title":"Predicting the Unpredictable: The Magic of Mixture Density Networks Explained","text":"<p>Tired of your neural networks making lame predictions? \ud83e\udd26\u200d\u2642\ufe0f Wish they could predict more than just the average future? Enter Mixture Density Networks (MDNs), a supercharged approach that doesn\u2019t just guess the future \u2014 it predicts a whole spectrum of possibilities!</p> <p>Condensed mini\u2011blog from my piece on Mixture Density Networks for uncertainty-aware regression.</p> <p>Read full article on Medium.</p>"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/#how-an-mdn-works-in-one-gulp","title":"How an MDN Works (in one gulp)","text":"<p>Given input \\(x\\), the network outputs:</p> <ul> <li>Mixture weights \\(\\alpha_k(x)\\), via softmax so they sum to 1</li> <li>Means \\(\\mu_k(x)\\)</li> <li>Standard deviations \\(\\sigma_k(x)\\), via exp to keep them positive</li> </ul> <p>Then the conditional density is:</p> <p>\\(p(t\\mid x) = \\sum_{k=1}^{K} \\alpha_k(x)\\, \\mathcal{N}\\big(t\\;\\big|\\;\\mu_k(x),\\, \\sigma_k^2(x)\\big).\\)</p>"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/#training-maximize-likelihood","title":"Training = Maximize Likelihood","text":"<p>We minimize negative log-likelihood (NLL) over the dataset \\(\\{(x_i, t_i)\\}\\):</p> <p>\\(\\mathcal{L} = - \\sum_i \\log\\Big[\\sum_k \\alpha_k(x_i)\\, \\mathcal{N}\\big(t_i\\;\\big|\\;\\mu_k(x_i),\\, \\sigma_k^2(x_i)\\big)\\Big].\\)</p> <p>This pushes the right components to \u201cown\u201d the right regions while learning both where mass should live (\\(\\mu\\)) and how uncertain it is (\\(\\sigma\\)).</p>"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/#a-compact-pytorch-mdn","title":"A Compact PyTorch MDN","text":"<p>Below is a tidy version of the loss and head you can drop into a regressor. (See the original for a full training loop and dataset plumbing.)</p> <pre><code># Loss for a Gaussian Mixture output\n# alpha: (N, K), sigma: (N, K, T), mu: (N, K, T), target: (N, T)\nimport torch, torch.nn.functional as F\n\ndef mdn_loss(alpha, sigma, mu, target, eps=1e-8):\n    target = target.unsqueeze(1).expand_as(mu)          # (N, 1, T) -&gt; (N, K, T)\n    m = torch.distributions.Normal(loc=mu, scale=sigma) # component log-probs\n    log_prob = m.log_prob(target).sum(dim=2)            # (N, K)\n    log_alpha = torch.log(alpha + eps)                  # avoid log(0)\n    loss = -torch.logsumexp(log_alpha + log_prob, dim=1)\n    return loss.mean()\n</code></pre> <pre><code># Minimal MDN head\nimport torch.nn as nn\n\nclass MDN(nn.Module):\n    def __init__(self, in_dim, out_dim, hidden, K):\n        super().__init__()\n        self.backbone = nn.Sequential(\n            nn.Linear(in_dim, hidden), nn.Tanh(),\n            nn.Linear(hidden, hidden), nn.Tanh(),\n        )\n        self.z_alpha = nn.Linear(hidden, K)\n        self.z_sigma = nn.Linear(hidden, K * out_dim)\n        self.z_mu    = nn.Linear(hidden, K * out_dim)\n        self.K = K\n        self.out_dim = out_dim\n\n    def forward(self, x):\n        h = self.backbone(x)\n        alpha = F.softmax(self.z_alpha(h), dim=-1)\n        sigma = torch.exp(self.z_sigma(h)).view(-1, self.K, self.out_dim)\n        mu    = self.z_mu(h).view(-1, self.K, self.out_dim)\n        return alpha, sigma, mu\n</code></pre>"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/#sampling-predictions","title":"Sampling Predictions","text":"<p>Turn mixture params into concrete draws to visualize possible futures:</p> <pre><code>import itertools\n\ndef sample_mdn(alpha, sigma, mu, samples=10):\n    N, K, T = mu.shape\n    preds = torch.zeros(N, samples, T)\n    u = torch.rand(N, samples)\n    csum = alpha.cumsum(dim=1)\n    for i, j in itertools.product(range(N), range(samples)):\n        k = torch.searchsorted(csum[i], u[i, j]).item()\n        preds[i, j] = torch.normal(mu[i, k], sigma[i, k])\n    return preds  # (N, samples, T)\n</code></pre>"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/#quick-case-study-apparent-temperature","title":"Quick Case Study: \u201cApparent Temperature\u201d \ud83c\udf21\ufe0f","text":"<p>Train an MDN (e.g., two hidden tanh layers of width \\~50) on a simple weather dataset to predict apparent temperature. You\u2019ll get both accurate central tendencies and sensible spread. Typical diagnostics:</p> <ul> <li>R\u00b2 near 0.99 (with careful preprocessing)</li> <li>MAE \u2248 0.5 degrees</li> <li>Histograms and scatter plots show measured vs. sampled predictions aligning closely</li> </ul> \u00d7 <p>Pro tips: remove outliers, consider resampling, and tune \\(K\\) and hidden width. Larger \\(K\\) gives more expressivity but can make training trickier.</p>"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/#when-to-reach-for-mdns","title":"When to Reach for MDNs","text":"<ul> <li>Targets with multiple valid outcomes for the same input (multi\u2011modal)</li> <li>Aleatoric uncertainty that varies with \\(x\\) (heteroscedastic noise)</li> <li>You care about full predictive distributions (not just point estimates)</li> <li>Examples: motion forecasting, demand spikes, sensor fusion, finance tails, weather nowcasting</li> </ul>"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/#gotchas-good-habits","title":"Gotchas &amp; Good Habits","text":"<ul> <li>Stability: add small epsilons; clamp/log\u2011sum\u2011exp as above.</li> <li>Initialization: start with smaller \\(K\\); increase once training is stable.</li> <li>Evaluation: don\u2019t just check RMSE\u2014use NLL, CRPS, calibration curves, and coverage of prediction intervals.</li> <li>Inference: report means, modes, and quantiles from the mixture; visualize multiple samples.</li> </ul>"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/#wrapup","title":"Wrap\u2011Up","text":"<p>MDNs bolt a probability distribution to your neural net, turning point predictions into a palette of possibilities. If your target is messy, multi\u2011peaked, or just plain chaotic, MDNs are a pragmatic, PyTorch\u2011friendly way to model \u201cthe unpredictable\u201d\u2014and say how confident you are.</p>"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/#read-the-full-article","title":"\ud83d\udcd6 Read the Full Article","text":"Predicting the Unpredictable:The Magic of Mixture Density Networks Explained <p>Deep dive into predictive modeling techniques and strategies for handling uncertain outcomes in data science.</p>          \ud83d\udcd6 Full article available on Medium"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/","title":"From Skeptic to Believer: Unpacking the Model Context Protocol","text":"<p>I\u2019ll admit it , when I first heard about the Model Context Protocol (MCP), my eyes nearly rolled out of my head. Having worn the data scientist hat for over a decade, I\u2019ve watched plenty of hyped frameworks come and go. But curiosity got the better of me. I dug in, tried it out, and well, I was pleasantly surprised. It turns out MCP actually is useful!</p> <p>Condensed mini\u2011blog from my piece on the Model Context Protocol (MCP).</p> <p>Read full article on Medium.</p>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#mcp-in-one-breath","title":"MCP in one breath","text":"<p>It\u2019s a protocol, not a framework. Think USB\u2011C for LLM apps.</p> <ul> <li>Host: your LLM application (chatbot, editor plugin, desktop app). The decider.</li> <li>Client: the in\u2011app component that speaks MCP to servers. The messenger.</li> <li>Server: exposes Resources (read\u2011only data), Tools (actions), and Prompts (reusable templates). The do\u2011er.</li> </ul> <p>One server can power many apps; one app can connect to many servers. No model/vendor lock\u2011in.</p>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#the-three-primitives-with-bitesized-code","title":"The three primitives (with bite\u2011sized code)","text":""},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#1-resources-get-me-context","title":"1) Resources \u2014 \"GET me context\"","text":"<p>Read\u2011only handles to data you want the model to see.</p> <pre><code>from mcp.server.fastmcp import FastMCP\nmcp = FastMCP(\"My App\")\n\n@mcp.resource(\"config://app\")\ndef get_config() -&gt; str:\n    return \"App configuration here\"\n\n@mcp.resource(\"users://{user_id}/profile\")\ndef get_user_profile(user_id: str) -&gt; str:\n    return f\"Profile data for user {user_id}\"\n</code></pre> <p>Use for configs, user metadata, document contents, preloaded business context.</p>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#2-tools-do-the-thing","title":"2) Tools \u2014 \"Do the thing\"","text":"<p>Side\u2011effectful actions or computations (sync/async).</p> <pre><code>import httpx\nfrom mcp.server.fastmcp import FastMCP\nmcp = FastMCP(\"My App\")\n\n@mcp.tool()\ndef calculate_bmi(weight_kg: float, height_m: float) -&gt; float:\n    return weight_kg / (height_m ** 2)\n\n@mcp.tool()\nasync def fetch_weather(city: str) -&gt; str:\n    async with httpx.AsyncClient() as client:\n        r = await client.get(f\"https://api.weather.com/{city}\")\n        return r.text\n</code></pre> <p>Great for API calls, business logic, CRUD, automation.</p>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#3-prompts-stop-rewriting-templates","title":"3) Prompts \u2014 \"Stop rewriting templates\"","text":"<p>Server\u2011defined, reusable prompt patterns.</p> <pre><code>from mcp.server.fastmcp import FastMCP\nfrom mcp.server.fastmcp.prompts import base\nmcp = FastMCP(\"My App\")\n\n@mcp.prompt()\ndef review_code(code: str) -&gt; str:\n    return f\"Please review this code:\\n\\n{code}\"\n\n@mcp.prompt()\ndef debug_error(error: str) -&gt; list[base.Message]:\n    return [\n        base.UserMessage(\"I'm seeing this error:\"),\n        base.UserMessage(error),\n        base.AssistantMessage(\"I'll help debug that. What have you tried so far?\"),\n    ]\n</code></pre> <p>Handy for code review, support workflows, query templates, standardized outputs.</p>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#a-practical-build-wrap-an-mlflow-model-with-mcp","title":"A practical build: wrap an MLflow model with MCP","text":"<p>Goal: expose a wine\u2011quality predictor (served via MLflow) to any MCP\u2011compatible host (e.g., Claude Desktop).</p> <p>Tool: <code>predict_wine_quality</code></p> <pre><code># server.py\n@mcp.tool(name=\"predict_wine_quality\", description=\"Predict wine quality using MLflow API\")\nasync def predict_wine_quality(inputs: list[list[float]], columns: list[str]) -&gt; list[float]:\n    payload = {\"dataframe_split\": {\"data\": inputs, \"columns\": columns}}\n    async with httpx.AsyncClient() as client:\n        resp = await client.post(MLFLOW_URL, json=payload)\n    return resp.json()[\"predictions\"]\n</code></pre> <p>Resource: example payload to guide users</p> <pre><code>@mcp.resource(\n    uri=\"wine://example\",\n    name=\"wine_quality_example\",\n    description=\"Example wine quality inputs and outputs\",\n    mime_type=\"application/json\",\n)\ndef get_input_example() -&gt; str:\n    import json\n    return json.dumps({\n        \"columns\": [\n            \"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\",\n            \"chlorides\", \"free sulfur dioxide\", \"total sulfur dioxide\",\n            \"density\", \"pH\", \"sulphates\", \"alcohol\"\n        ],\n        \"data\": [\n            [7.4, 0.7, 0, 1.9, 0.076, 11, 34, 0.9978, 3.51, 0.56, 9.4],\n            [7.8, 0.88, 0, 2.6, 0.098, 25, 67, 0.9968, 3.2, 0.68, 9.8]\n        ]\n    }, indent=2)\n</code></pre> <p>Prompt: tidy the output for chat</p> <pre><code>@mcp.prompt(name=\"format_predictions\", description=\"Format wine quality predictions for chatbot\")\ndef format_predictions(predictions: list[float]) -&gt; str:\n    formatted = \"\\n\".join(f\"- Sample {i+1}: **{s:.2f}/10**\" for i, s in enumerate(predictions))\n    return f\"## Predicted Wine Quality Scores\\n\\n{formatted}\"\n</code></pre>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#dev-ux-test-containerize-connect","title":"Dev UX: test, containerize, connect","text":"<p>Local dev &amp; inspector</p> <pre><code>mcp dev src/mcp_server_mlflow/server.py\n</code></pre> <p>Containerize it (recommended)</p> <pre><code>FROM python:3.11-slim-bookworm\nWORKDIR /app\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\n    build-essential curl git &amp;&amp; rm -rf /var/lib/apt/lists/*\nCOPY pyproject.toml uv.lock README.md ./\nCOPY src/ ./src/\nRUN pip install uv &amp;&amp; uv venv &amp;&amp; uv sync\nRUN useradd -ms /bin/bash appuser\nUSER appuser\nENV PATH=\"/app/.venv/bin:$PATH\"\nENV MLFLOW_URL=\"http://host.docker.internal:1234/invocations\"\nENTRYPOINT [\"mcp-server-mlflow\"]\n</code></pre> <p>Build it:</p> <pre><code>docker build -t mcp/wine .\n</code></pre> <p>Wire it to Claude Desktop (or any MCP host)</p> <p>Option A \u2014 Docker:</p> <pre><code>{\n  \"mcpServers\": {\n    \"My Wine Quality Server (docker)\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"--init\", \"mcp/wine\"]\n    }\n  }\n}\n</code></pre> <p>Option B \u2014 <code>uv</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"My Wine Quality Server (uv)\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"/PATH/TO/PROJECT\", \"run\", \"mcp-server-mlflow\"]\n    }\n  }\n}\n</code></pre>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#when-mcp-shines-and-when-its-overkill","title":"When MCP shines (and when it\u2019s overkill)","text":"<p>Use MCP when:</p> <ul> <li>You have multiple apps/agents that need the same tools/data/prompts.</li> <li>You want model\u2011agnostic integrations (swap providers without rewiring).</li> <li>Teams should share capabilities via a common, versionable interface.</li> </ul> <p>Skip MCP when:</p> <ul> <li>It\u2019s a one\u2011off script with a single hardcoded API call.</li> <li>There\u2019s no reuse across apps or teammates.</li> </ul>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#takeaways","title":"Takeaways","text":"<ul> <li>MCP standardizes how LLM apps access resources, tools, and prompts.</li> <li>It trades brittle per\u2011app glue for composable, reusable, portable servers.</li> <li>The Python SDK + <code>mcp dev</code> + Docker make it straightforward to ship.</li> </ul>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#wrapup","title":"Wrap\u2011up","text":"<p>I started a skeptic. After wiring a real MLflow model through MCP and dropping it into a chat host, I\u2019m\u2026 converted. If your LLM features are stuck in \u201cautocomplete\u201d mode, MCP is the cleanest path I\u2019ve seen to make them world\u2011aware without bespoke spaghetti.</p>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#read-the-full-article","title":"\ud83d\udcd6 Read the Full Article","text":"From Skeptic to Believer: Unpacking the Model Context Protocol <p>MCP is a powerful open protocol that lets LLM apps connect to tools and data sources in a standardized, plug-and-play way\u2014kind of like REST for AI.</p>          \ud83d\udcd6 Full article available on Medium"},{"location":"blog/2026/01/12/skills-commands-sub-agents-its-all-the-same-thing-now/","title":"Skills, Commands, Sub-agents... It's All the Same Thing Now!","text":"<p>Claude Code used to give us a neat little taxonomy:</p> <ul> <li>Commands: quick, explicit \u201csaved prompt\u201d shortcuts (<code>/test</code> \u2192 runs a prompt).</li> <li>Skills: richer procedural knowledge packs that the model could auto-discover based on intent.</li> <li>Sub-agents: \u201cpersonas\u201d that worked in a forked context so your main conversation stayed clean.</li> </ul> <p>It was tidy. It was elegant. It was\u2026 doomed.</p> <p>Condensed mini-blog from my piece on how Claude Code\u2019s recent updates collapsed 3 abstractions into 1 (and why that\u2019s fine)</p> <p>Read the full article here</p>"},{"location":"blog/2026/01/12/skills-commands-sub-agents-its-all-the-same-thing-now/#the-old-world-three-lanes-no-overlap","title":"The old world: three lanes, no overlap","text":"<p>Back then, the decision was pretty straightforward:</p> <ul> <li>Want a shortcut you manually trigger? Command.</li> <li>Want a reusable procedure the model can find and apply? Skill.</li> <li>Want an \u201cexpert worker\u201d persona that runs separately? Sub-agent.</li> </ul> <p>You could actually draw a decision tree and not hate yourself.</p>"},{"location":"blog/2026/01/12/skills-commands-sub-agents-its-all-the-same-thing-now/#the-update-that-broke-everyones-mental-model","title":"The update that broke everyone\u2019s mental model","text":"<p>Then the recent Claude Code updates arrived and casually knocked down the fences:</p>"},{"location":"blog/2026/01/12/skills-commands-sub-agents-its-all-the-same-thing-now/#1-skills-can-be-invoked-like-commands","title":"1) Skills can be invoked like Commands","text":"<p>Skills aren\u2019t only \u201cauto-discovered\u201d anymore. You can now explicitly run a Skill with a slash, just like a Command.</p> <p>That erases one of the main functional differences.</p>"},{"location":"blog/2026/01/12/skills-commands-sub-agents-its-all-the-same-thing-now/#2-skills-can-fork-context-like-sub-agents","title":"2) Skills can fork context like Sub-agents","text":"<p>Skills also gained a YAML front matter option called:</p> <pre><code>context_fork: true\n</code></pre> <p>Meaning a Skill can run in its own independent context window \u2014 the signature move of sub-agents.</p> <p>So now:</p> <ul> <li>A Skill can act like a Command (explicit invocation)</li> <li>A Skill can act like a Sub-agent (forked context)</li> <li>A Sub-agent is basically a prompt that can load Skills</li> </ul> <p>At some point you stop asking \u201cwhich abstraction is correct?\u201d and start asking \u201care we just naming the same thing three times?\u201d</p>"},{"location":"blog/2026/01/12/skills-commands-sub-agents-its-all-the-same-thing-now/#a-mental-model-that-still-works","title":"A mental model that still works","text":"<p>When implementation details blur, the only way to stay sane is to categorize by intent, not mechanics.</p>"},{"location":"blog/2026/01/12/skills-commands-sub-agents-its-all-the-same-thing-now/#skills-knowledge-packs-sops","title":"Skills = Knowledge Packs (SOPs)","text":"<p>Treat Skills like Standard Operating Procedures \u2014 reusable capabilities and step libraries.</p> <p>Use a Skill when you have something like:</p> <ul> <li>a specific script/workflow (e.g., \u201cparse a PDF and extract tables\u201d),</li> <li>a checklist (e.g., \u201ccode review steps\u201d),</li> <li>a repeatable procedure that should exist independently of any persona.</li> </ul> <p>Practical tip: model your Skill as a folder: - <code>SKILL.md</code> - any resources it needs (snippets, templates, helper files)</p>"},{"location":"blog/2026/01/12/skills-commands-sub-agents-its-all-the-same-thing-now/#sub-agents-personas-workers","title":"Sub-agents = Personas (Workers)","text":"<p>Sub-agents are the identity layer \u2014 conceptual knowledge, role, worldview.</p> <p>Use a Sub-agent when you want:</p> <ul> <li>\u201cSecurity Engineer\u201d energy for a review,</li> <li>\u201cQA Tester\u201d behavior to drive a test run,</li> <li>an agent that should work in a contained context (especially useful when it goes off exploring).</li> </ul>"},{"location":"blog/2026/01/12/skills-commands-sub-agents-its-all-the-same-thing-now/#the-composition-tools-workers","title":"The composition: tools + workers","text":"<p>The punchline is simple:</p> <p>Skills are the tools. Sub-agents are the workers who wield them.</p> <p>Build a \u201cCode Reviewer\u201d sub-agent that loads: - a \u201cSecurity Review\u201d Skill - a \u201cStyle Guide\u201d Skill</p> <p>Then let that persona execute in a forked context while your main workspace stays uncluttered.</p>"},{"location":"blog/2026/01/12/skills-commands-sub-agents-its-all-the-same-thing-now/#wrap-up-the-vocabulary-is-messy-your-architecture-doesnt-have-to-be","title":"Wrap-up: the vocabulary is messy, your architecture doesn\u2019t have to be","text":"<p>Claude Code\u2019s feature set is in a transition period: powerful, flexible, and a little linguistically chaotic. The old \u201cchoose Command vs Skill vs Sub-agent\u201d decision trees don\u2019t apply anymore because Skills absorbed everyone\u2019s superpowers.</p> <p>So instead:</p> <ul> <li>Organize by purpose: procedures go in Skills, identities go in Sub-agents.</li> <li>Compose: load Skills into Sub-agents like tools into a toolkit.</li> <li>Use context forking intentionally: keep exploratory or noisy work in a separate window.</li> </ul> <p>Try it: take one workflow you already use, refactor it into a Skill (procedure) plus a Sub-agent (persona) that loads it. Your future self will thank you \u2014 quietly, from a cleaner context window.</p>"},{"location":"blog/2026/01/12/skills-commands-sub-agents-its-all-the-same-thing-now/#read-the-full-article","title":"\ud83d\udcd6 Read the Full Article","text":"Skills, Commands, Sub-Agents... it's all the same thing now! <p>How Claude Code's recent updates collapsed three abstractions into one (and why that's fine).</p>          \ud83d\udcd6 Full article available on Substack"},{"location":"datavengers/","title":"About Datavengers\u2122","text":"<p>Datavengers SAS is a data, analytics, and artificial intelligence company founded by Dr Miguel Miranda Dias to help businesses integrate AI and data into their operations. Our services include AI strategy development, team training, tool selection, and custom AI integration.</p> <p>With a mission to accelerate AI adoption, Datavengers also provides educational resources, beginner-friendly courses, and a business accelerator for tech professionals. To learn more, visit our website.</p> <p>Visit website </p>"},{"location":"portfolio/","title":"Featured Projects","text":"<p>Welcome to my portfolio of data science and AI projects. Each project demonstrates my expertise in delivering impactful solutions to real-world business challenges.</p> <ul> <li> <p>Patent Trend Analysis with LLMs</p> <p>End-to-end AI pipeline to surface EV-battery technology trends for IP and innovation teams.</p> </li> <li> <p>Healthcare Complaint Processing System</p> <p>Automated multi-channel complaint intake and insights for a private healthcare provider.</p> </li> <li> <p>Secure Enterprise AI Assistant</p> <p>Confidential ChatGPT-like assistant with RAG, translation, and secure on-premises deployment for 300+ employees.</p> </li> <li> <p>AI Executive Assistant for Email and Calendar</p> <p>Automated scheduling and email workflows through Slack and Nylas, cutting executive admin time by 50%.</p> </li> <li> <p>LLM-Based Clinical Decision Support System</p> <p>RAG-powered decision support that surfaced trusted diagnoses and cut reference search time by 60%.</p> </li> <li> <p>AI-Powered Ergonomic Posture Tracking System</p> <p>Automated ergonomic scoring from depth-video, cutting plant-wide studies from 12 months to 3 weeks.</p> </li> </ul>"},{"location":"portfolio/projects/project-1-ergonomic-assessment/","title":"AI-Powered Ergonomic Assessment with 3D Skeleton Tracking","text":"<p>Case Study Summary</p> <p>Client: Confidential / Fortune 500 European Automotive Supplier Industry: Automotive (Manufacturing) Role: Lead Data Scientist</p> <p>Impact Metrics:</p> <ul> <li>\u221294% study duration per plant (12 months \u2192 3 weeks)</li> <li>\u224817\u00d7 throughput increase (workstations assessed per week)</li> <li>Real-time in-session ergonomic scoring</li> <li>Zero post-hoc manual angle measurement</li> <li>Standardized scoring rubric across sites</li> </ul> <p>The ergonomics team needed plant-wide ergonomics assessments in weeks, not months; we delivered an AI-powered 3D vision workflow that lets ergonomists identify high risk workstations quickly and consistently at plant scale.</p>"},{"location":"portfolio/projects/project-1-ergonomic-assessment/#the-challenge","title":"The Challenge","text":"<p>Manual video review and angle measurement made ergonomic studies slow and error-prone. Plants had hundreds of workstations and repetitive tasks with musculoskeletal risk. One plant needed roughly a year to complete a full study, delaying mitigation and inflating risk exposure.</p>"},{"location":"portfolio/projects/project-1-ergonomic-assessment/#the-solution","title":"The Solution","text":""},{"location":"portfolio/projects/project-1-ergonomic-assessment/#implementation","title":"\u2192 Implementation","text":"<p>I built a depth-camera pipeline that converts 3D video into live skeletal keypoints and joint kinematics, then maps those to an ergonomics scoring rubric. The pipeline executes in stages: capture \u2192 3D pose estimation \u2192 kinematic features (angles/rotations) \u2192 rules engine (rubric lookup) \u2192 real-time score &amp; report. The system delivered in-session feedback and standardized outputs the ergonomics team could act on immediately.</p>"},{"location":"portfolio/projects/project-1-ergonomic-assessment/#solution-architecture","title":"\u2192 Solution Architecture","text":"<p>Depth camera \u2192 skeletal tracking SDK \u2192 kinematics engine \u2192 rules/scoring service \u2192 dashboard/report export.</p>"},{"location":"portfolio/projects/project-1-ergonomic-assessment/#tech-stack","title":"\u2192 Tech Stack","text":"<ul> <li>Cloud: Microsoft Azure Cloud Infrastructure</li> <li>CI/CD: Azure DevOps Pipelines</li> <li>Containerization: Docker</li> <li>Pose Detection: Azure Kinect Body Tracking SDK (C#, 3D joints), Intel RealSense depth</li> <li>Application/UI: Node.js app handling capture control, live overlays, and operator workflow</li> <li>Backend: C# SDK bridge for camera integration; Python modules for kinematics and scoring</li> <li>Kinematics &amp; Scoring: Python modules for joint angles, ranges of motion, and rotations</li> <li>Rules Engine: Configurable rubric lookup for 7 risk classes (1: optimal, 7: not acceptable)</li> <li>Data Handling: On-device processing; no personal data retention (GDPR compliance)</li> </ul>"},{"location":"portfolio/projects/project-1-ergonomic-assessment/#key-learnings","title":"Key Learnings","text":"<ul> <li>Real-time Edge Processing: Optimized 3D pose estimation and kinematic calculations to run locally on workstations, enabling immediate feedback.</li> <li>GDPR Compliance: Designed the system to process video data in-memory without retention, storing only anonymous skeletal data and scores to satisfy strict privacy rules.</li> <li>Cross-Disciplinary Validation: Collaborated deeply with ergonomics experts to translate subjective scoring sheets into deterministic geometric rules.</li> <li>Standardization: Replaced subjective human observation with consistent, objective measurements across multiple global plant sites.</li> </ul>"},{"location":"portfolio/projects/project-1-ergonomic-assessment/#measurable-impact","title":"Measurable Impact","text":"<ul> <li>\u221294% Study Time: Plant-wide assessment time reduced from 12 months to 3 weeks.</li> <li>Real-time Efficiency: Live scoring during capture eliminated manual frame-by-frame measurements.</li> <li>Proactive Safety: Faster identification of high-risk stations enabled earlier redesign and training.</li> <li>Innovation: Reusable pipeline enabled adjacent analytics, including cycle-time study work that contributed to a patent filing.</li> </ul> <p> Let's have a virtual coffee together!</p> <p>Want to see if we're a match? Let's have a chat and find out. Schedule a free 30-minute strategy session to discuss your AI challenges and explore how we can work together.</p> <p>Book Intro Call </p>"},{"location":"portfolio/projects/project-2-local-chatbot/","title":"Secure Enterprise AI Assistant","text":"<p>Case Study Summary</p> <p>Client: Confidential / Leading European EV Battery Manufacturer Industry: Automotive (EV Battery Manufacturing) Role: AI Tech Lead</p> <p>Impact Metrics:</p> <ul> <li>Eliminated risk of data leakage from external AI tools (100% local processing)</li> <li>Reduced onboarding document review time by ~70%</li> <li>Scaled seamlessly from 20 \u2192 300 concurrent users with &lt;2s latency</li> <li>Enabled secure multilingual translation across 5+ languages</li> <li>Continuous ingestion of company news and onboarding documents</li> </ul> <p>Built a secure, enterprise-grade AI assistant, a ChatGPT-like tool deployed fully on-premises. The system combined a deterministic RAG pipeline using a centralized knowledge base, real-time translation, and a React-based UI. It enabled global employees to interact with a private AI assistant without leaking any sensitive information.</p>"},{"location":"portfolio/projects/project-2-local-chatbot/#the-challenge","title":"The Challenge","text":"<p>Employees were increasingly using public tools (ChatGPT, Google Translate) for document digestion and translation. This posed two core risks: 1. Data leakage to external AI platforms 2. Inconsistent document analysis quality</p> <p>The client required a fully local AI assistant, scalable, multilingual, and compliant with internal data governance.</p>"},{"location":"portfolio/projects/project-2-local-chatbot/#the-solution","title":"The Solution","text":""},{"location":"portfolio/projects/project-2-local-chatbot/#implementation","title":"\u2192 Implementation","text":"<p>I implemented a secure hybrid architecture with full separation of concerns. A React-based frontend with SSO login managed user sessions and chat interactions, while a dedicated user database stored chat history. A vector database (PostgreSQL + pgvector) held embedded company knowledge, optimized for RAG use. Distributed Ollama LLM endpoints were deployed across a local 9-GPU HPC cluster, and event-driven pipelines handled ingestion of documents, embedding generation, translation, and news updates. Each component was isolated and containerized, enabling scalable, fault-tolerant operations while enforcing data boundaries between chat storage and knowledge retrieval.</p>"},{"location":"portfolio/projects/project-2-local-chatbot/#solution-architecture","title":"\u2192 Solution Architecture","text":"<p>A secure local AI assistant architecture using a React-based UI, user DB, Ollama endpoints on 9x Nvidia GPUs HPC, and pgvector-based RAG pipelines.</p>"},{"location":"portfolio/projects/project-2-local-chatbot/#tech-stack","title":"\u2192 Tech Stack","text":"<ul> <li>Infrastructure: On-prem HPC cluster (9x Nvidia GPUs)</li> <li>Frontend: React-based UI with SSO</li> <li>LLM Runtime: Ollama endpoints (local, GPU distributed)</li> <li>Vector DB: PostgreSQL + pgvector</li> <li>User DB: PostgreSQL (chat metadata, settings)</li> <li>Ingestion Pipelines: Python-based, event-triggered</li> <li>Containerization &amp; Scaling: Docker, optionally Kubernetes (bare-metal)</li> </ul>"},{"location":"portfolio/projects/project-2-local-chatbot/#key-learnings","title":"Key Learnings","text":"<ul> <li>Data Sovereignty: Complete local deployment eliminated external dependencies and data leakage risks.</li> <li>Scalability: Distributed inference on GPU clusters maintained low latency (&lt;2s) even with 300+ concurrent users.</li> <li>Data Hygiene: Separating user metadata from vector knowledge prevented knowledge base pollution.</li> <li>User Adoption: Integrating with existing SSO and providing a familiar UI (React-based) smoothed the transition from public tools.</li> </ul>"},{"location":"portfolio/projects/project-2-local-chatbot/#measurable-impact","title":"Measurable Impact","text":"<ul> <li>Achieved complete local deployment, removing all dependency on external AI services</li> <li>Improved onboarding efficiency by ~70% through instant document summarization</li> <li>Prevented knowledge base \"pollution\" by separating user metadata from vector knowledge</li> <li>Maintained &lt;2s latency for 300+ concurrent users via GPU-distributed inference endpoints</li> <li>Enabled fast, private multilingual translation across departments</li> </ul> <ul> <li> <p> Let's have a virtual coffee together!</p> <p>Want to see if we're a match? Let's have a chat and find out. Schedule a free 30-minute strategy session to discuss your AI challenges and explore how we can work together.</p> <p>Book Intro Call </p> </li> </ul>"},{"location":"portfolio/projects/project-3-patent-trend-analytics/","title":"Patent Trend Analysis with LLMs","text":"<p>Case Study Summary</p> <p>Client: Confidential / Leading European EV Battery Manufacturer Industry: Automotive (EV batteries) Role: AI Tech Lead</p> <p>Impact Metrics:</p> <ul> <li>Analysts reallocated from manual tagging to higher-value trend interpretation</li> <li>-90% review/labeling time per weekly batch (from 5 days \u2192 0.5 day)  </li> <li>\u2248 \u20ac490k/year cost savings (based on 10 IP engineers' time saved)</li> </ul> <p>The IP team needed faster, consistent insight into thousands of new patents to inform R&amp;D on emerging EV-battery technologies. I delivered a pipeline that turns raw patent exports into titled clusters with summaries and a trends dashboard.</p>"},{"location":"portfolio/projects/project-3-patent-trend-analytics/#the-challenge","title":"The Challenge","text":"<p>IP engineers were manually scanning and tagging multilingual patents\u2014slow, inconsistent, and hard to replicate at scale. Multilingual content and unstructured abstracts made it difficult to compare filings and report consolidated trends to innovation stakeholders.</p>"},{"location":"portfolio/projects/project-3-patent-trend-analytics/#the-solution","title":"The Solution","text":""},{"location":"portfolio/projects/project-3-patent-trend-analytics/#implementation","title":"\u2192 Implementation","text":"<p>I built REST endpoints for language detection \u2192 translation \u2192 domain-specific embedding \u2192 unsupervised clustering \u2192 cluster summarization \u2192 trend tracking, exposed via a FastAPI service on Azure and backed by a vector index for semantic lookups. Clusters receive concise titles/summaries and can optionally align to IPC categories for consistent reporting.</p>"},{"location":"portfolio/projects/project-3-patent-trend-analytics/#solution-architecture","title":"\u2192 Solution Architecture","text":"<p>Baseline EV-battery patent trend analytics solution architecture</p>"},{"location":"portfolio/projects/project-3-patent-trend-analytics/#tech-stack","title":"\u2192 Tech Stack","text":"<ul> <li>Cloud: Microsoft Azure Cloud Infrastructure</li> <li>CI/CD: Azure DevOps Pipelines</li> <li>Containerization: Docker</li> <li>Data Platform: Azure Databricks (APIs for ingestion &amp; jobs)</li> <li>Vector Index: Databricks Vector Search</li> <li>Backend: Python services with FastAPI (REST)</li> <li>Language Detection: XLM-Roberta</li> <li>Translation: mBART-large-50 many to one</li> <li>Embeddings: BERT for Patents (fine-tuned for EV-battery patents)</li> <li>LLM (summarization): Mistral 7B</li> </ul>"},{"location":"portfolio/projects/project-3-patent-trend-analytics/#key-learnings","title":"Key Learnings","text":"<ul> <li>Multilingual filings: Used XLM-RoBERTa + mBART-50 to standardize language before embedding.</li> <li>Noisy abstracts &amp; jargon: Fine-tuned domain embeddings (BERT for Patents) to boost semantic cohesion before clustering.</li> <li>Inconsistent labels: Auto-titled clusters with Mistral-7B; optionally mapped to IPC-labels.</li> <li>Scalability &amp; repeatability: Orchestrated Databricks jobs with containerized services and Azure DevOps CI/CD for stable weekly runs.</li> <li>Analyst adoption: Added concise summaries + trends dashboard, shifting effort from manual tagging to interpretation.</li> </ul>"},{"location":"portfolio/projects/project-3-patent-trend-analytics/#measurable-impact","title":"Measurable Impact","text":"<ul> <li>Earlier visibility of emerging battery-tech themes; analysts redeployed to higher-value analysis.</li> <li>Analysts reallocated from manual tagging to higher-value trend interpretation</li> <li>-90% review/labeling time per weekly batch (from 5 days \u2192 0.5 day)  </li> <li>\u2248 \u20ac490k/year cost savings (based on 10 IP engineers' time saved alone)</li> </ul> <ul> <li> <p> Let's have a virtual coffee together!</p> <p>Want to see if we're a match? Let's have a chat and find out. Schedule a free 30-minute strategy session to discuss your AI challenges and explore how we can work together.</p> <p>Book Intro Call </p> </li> </ul>"},{"location":"portfolio/projects/project-4-clinical-decision-support/","title":"LLM-Based Clinical Decision Support System","text":"<p>Case Study Summary</p> <p>Client: Confidential Industry: Healthcare Role: AI Solution Architect</p> <p>Impact Metrics:</p> <ul> <li>90% of suggestions rated relevant by clinicians</li> <li>60% reduction in time spent finding references</li> <li>High adoption across pilot clinicians</li> <li>Transparent reasoning with source-linked citations</li> </ul> <p>I built a clinical decision support tool that lets providers submit patient context and receive evidence-backed diagnostic suggestions, summaries, and next-step recommendations grounded in the latest medical guidelines.</p>"},{"location":"portfolio/projects/project-4-clinical-decision-support/#the-challenge","title":"The Challenge","text":"<p>Clinicians were spending too much time searching disparate guidelines and past notes before making diagnostic decisions. Rapidly changing medical knowledge made it difficult to stay current, and opaque AI outputs undermined trust in automated recommendations.</p>"},{"location":"portfolio/projects/project-4-clinical-decision-support/#the-solution","title":"The Solution","text":""},{"location":"portfolio/projects/project-4-clinical-decision-support/#implementation","title":"\u2192 Implementation","text":"<p>I implemented a privacy-first workflow that ingests patient details, anonymizes sensitive data, and enriches it with a retrieval-augmented pipeline. The system executes an input \u2192 retrieval \u2192 reasoning \u2192 summarization flow on Vertex AI, pulling curated medical sources before generating step-by-step clinical reasoning and suggested actions. A web dashboard surfaces summaries, cited references, and recommended tests directly in the clinician workflow.</p>"},{"location":"portfolio/projects/project-4-clinical-decision-support/#solution-architecture","title":"\u2192 Solution Architecture","text":"<p>Architecture diagram placeholder per request.</p>"},{"location":"portfolio/projects/project-4-clinical-decision-support/#tech-stack","title":"\u2192 Tech Stack","text":"<ul> <li>Infrastructure: Google Cloud (Vertex AI)</li> <li>Data Platform: BigQuery + Cloud Storage</li> <li>CI/CD: GitHub Actions</li> <li>Containerization: Docker + Cloud Run</li> <li>Backend: Python + FastAPI</li> <li>Modeling: LLM with retrieval-augmented generation</li> <li>Vector Index: Vertex AI Matching Engine</li> </ul>"},{"location":"portfolio/projects/project-4-clinical-decision-support/#key-learnings","title":"Key Learnings","text":"<ul> <li>Grounding: RAG dramatically improved suggestion relevance by anchoring responses to trusted sources.</li> <li>Transparency: Showing step-by-step reasoning built clinician confidence in model outputs.</li> <li>Freshness: Integrating curated, frequently updated guidelines kept recommendations current.</li> <li>Workflow fit: Surfacing citations and next steps inside the dashboard boosted adoption.</li> </ul>"},{"location":"portfolio/projects/project-4-clinical-decision-support/#measurable-impact","title":"Measurable Impact","text":"<ul> <li>90% of clinician ratings marked suggestions as relevant and helpful</li> <li>60% faster access to supporting literature for each case</li> <li>Higher adoption across pilot teams due to transparent reasoning</li> <li>Increased trust in AI recommendations through cited sources</li> </ul> <ul> <li> <p> Let's have a virtual coffee together!</p> <p>Want to see if we're a match? Let's have a chat and find out. Schedule a free 30-minute strategy session to discuss your AI challenges and explore how we can work together.</p> <p>Book Intro Call </p> </li> </ul>"},{"location":"portfolio/projects/project-5-email-calendar-assistant/","title":"AI Executive Assistant for Email and Calendar Management","text":"<p>Case Study Summary</p> <p>Client: Confidential Industry: Executive productivity Role: AI Solution Architect</p> <p>Impact Metrics:</p> <ul> <li>50% reduction in executive email and scheduling time</li> <li>95% accuracy in intent understanding and task execution</li> <li>Faster meeting coordination through Slack-based workflows</li> <li>Higher adoption from a conversational, natural-language interface</li> <li>Improved personalization from persisted context and preferences</li> </ul> <p>I built an AI executive assistant that connects Slack, email, and calendars so leaders can schedule meetings, triage inboxes, and draft responses without leaving their daily chat workflow.</p>"},{"location":"portfolio/projects/project-5-email-calendar-assistant/#the-challenge","title":"The Challenge","text":"<p>Executives were losing hours each week to scattered email threads and back-and-forth scheduling. The assistant needed to understand natural-language requests, coordinate across email and calendar providers, and respond in Slack with high accuracy.</p>"},{"location":"portfolio/projects/project-5-email-calendar-assistant/#the-solution","title":"The Solution","text":""},{"location":"portfolio/projects/project-5-email-calendar-assistant/#implementation","title":"\u2192 Implementation","text":"<p>I designed a FastAPI orchestration layer that receives Slack commands, enriches them with stored context in PostgreSQL, and routes actions to Nylas for calendar and email operations. A dedicated LLM step interprets intent and drafts responses, while confirmations flow back to Slack for a tight feedback loop.</p>"},{"location":"portfolio/projects/project-5-email-calendar-assistant/#solution-architecture","title":"\u2192 Solution Architecture","text":"<p>Slack messages flow into FastAPI, which orchestrates Nylas API calls for email and scheduling, uses an LLM for intent and drafting, persists context in PostgreSQL, and posts confirmations back to Slack.</p> <p>Architecture diagram placeholder per request.</p>"},{"location":"portfolio/projects/project-5-email-calendar-assistant/#tech-stack","title":"\u2192 Tech Stack","text":"<ul> <li>Infrastructure: Cloud-hosted API services</li> <li>Data Platform: PostgreSQL</li> <li>CI/CD: Not specified</li> <li>Containerization: Docker</li> <li>Backend: FastAPI</li> <li>Modeling: LLM-based intent parsing and drafting</li> <li>Vector Index: Not used</li> </ul>"},{"location":"portfolio/projects/project-5-email-calendar-assistant/#key-learnings","title":"Key Learnings","text":"<ul> <li>Workflow fit: Meeting users in Slack drove higher adoption and faster feedback.</li> <li>Context retention: Persisted preferences reduced repetitive clarification loops.</li> <li>API abstraction: Nylas simplified multi-provider email and calendar access.</li> <li>LLM guardrails: Structured prompts improved intent accuracy for scheduling.</li> </ul>"},{"location":"portfolio/projects/project-5-email-calendar-assistant/#measurable-impact","title":"Measurable Impact","text":"<ul> <li>Reduced executive time spent on email and scheduling by 50%.</li> <li>Achieved 95% accuracy on request understanding and execution.</li> <li>Increased daily usage with a Slack-first interaction model.</li> <li>Delivered more personalized responses using stored context and history.</li> </ul> <ul> <li> <p> Let's have a virtual coffee together!</p> <p>Want to see if we're a match? Let's have a chat and find out. Schedule a free 30-minute strategy session to discuss your AI challenges and explore how we can work together.</p> <p>Book Intro Call </p> </li> </ul>"},{"location":"portfolio/projects/project-6-complaint-processing-system/","title":"Healthcare Complaint Processing System","text":"<p>Case Study Summary</p> <p>Client: Confidential Industry: Private healthcare Role: AI Solution Architect</p> <p>Impact Metrics:</p> <ul> <li>70% reduction in manual complaint handling time</li> <li>91% complaint categorization accuracy</li> <li>Top 5 complaint themes identified within the first week</li> <li>Unified five intake channels into one processing pipeline</li> <li>Automated anonymization before analysis to meet privacy requirements</li> </ul> <p>I built an end-to-end complaint processing system that ingests text, voice, and handwritten submissions, normalizes and anonymizes them, and delivers clustered insights to a dashboard so private healthcare teams can prioritize responses quickly.</p>"},{"location":"portfolio/projects/project-6-complaint-processing-system/#the-challenge","title":"The Challenge","text":"<p>The entity received complaints from email, social-media platforms, voice, and handwritten letters, but every channel required different handling and manual triage. The team needed a single pipeline that could standardize content, protect personally identifiable information, and surface top themes without slowing response times.</p>"},{"location":"portfolio/projects/project-6-complaint-processing-system/#the-solution","title":"The Solution","text":""},{"location":"portfolio/projects/project-6-complaint-processing-system/#implementation","title":"\u2192 Implementation","text":"<p>I designed a staged pipeline (ingestion \u2192 normalization \u2192 anonymization \u2192 topic modeling \u2192 insights) on Google Cloud to keep processing consistent across formats. Cloud Functions orchestrated ingestion, Document AI handled OCR for handwritten letters, and Speech-to-Text converted voice messages into text. A small Vertex AI LLM standardized complaint language, clustered topics, and summarized themes for a lightweight web dashboard used by staff.</p>"},{"location":"portfolio/projects/project-6-complaint-processing-system/#solution-architecture","title":"\u2192 Solution Architecture","text":"<p>Architecture diagram placeholder per request.</p>"},{"location":"portfolio/projects/project-6-complaint-processing-system/#tech-stack","title":"\u2192 Tech Stack","text":"<ul> <li>Infrastructure: Google Cloud Platform</li> <li>Data Platform: Cloud Storage</li> <li>CI/CD: Cloud Build</li> <li>Containerization: Serverless Cloud Functions</li> <li>Backend: Cloud Functions, Cloud Run (dashboard API)</li> <li>Modeling: Vertex AI, Document AI, Speech-to-Text API</li> <li>Vector Index: Not used (topic clustering in Vertex AI)</li> </ul>"},{"location":"portfolio/projects/project-6-complaint-processing-system/#key-learnings","title":"Key Learnings","text":"<ul> <li>Multichannel normalization: Standardized text, voice, and OCR outputs early to keep downstream logic simple.</li> <li>Privacy compliance: Stripped PII before any LLM processing to meet regulatory requirements.</li> <li>Cost control: Chose a small LLM in Vertex AI to balance accuracy with operating costs.</li> <li>Operational relevance: Tailored cluster summaries to the language used by frontline teams.</li> </ul>"},{"location":"portfolio/projects/project-6-complaint-processing-system/#measurable-impact","title":"Measurable Impact","text":"<ul> <li>70% reduction in manual complaint handling time</li> <li>91% complaint categorization accuracy</li> <li>Top five complaint themes identified within the first week</li> <li>Faster prioritization of response plans using cluster summaries</li> </ul> <ul> <li> <p> Let's have a virtual coffee together!</p> <p>Want to see if we're a match? Let's have a chat and find out. Schedule a free 30-minute strategy session to discuss your AI challenges and explore how we can work together.</p> <p>Book Intro Call </p> </li> </ul>"},{"location":"portfolio/projects/project_template/","title":"<p>??? tip \"Portfolio Best Practices\" (don't include this section in the final portfolio)     This is a simplified example template. When creating your own portfolio:</p> <pre><code>- Include detailed technical challenges and how you solved them\n- Add specific metrics and KPIs that demonstrate impact\n- Show code snippets of interesting implementations\n- Include architecture diagrams and system designs\n- Document your decision-making process\n- Highlight your specific contributions to the project\n- Add visuals of the final product (if possible)\n</code></pre>  <p>Case Study Summary</p> <p>Client:  Industry:  Role:  <p>Impact Metrics:</p> <ul> <li> <li> <li>&lt;PRIMARY KPI 3 (e.g., &lt;2s latency)&gt;</li> <li> <li>   <p>","text":""},{"location":"portfolio/projects/project_template/#the-challenge","title":"The Challenge","text":"<p>&lt;2\u20133 sentences on the before-state: pain, constraints, and stakes. Mention data sources, scale, and business urgency.&gt;</p>"},{"location":"portfolio/projects/project_template/#the-solution","title":"The Solution","text":""},{"location":"portfolio/projects/project_template/#implementation","title":"\u2192 Implementation","text":"<p>&lt;2\u20134 sentences summarizing the solution strategy. Name the core stages (e.g., ingestion \u2192 processing \u2192 modeling \u2192 serving), key patterns (e.g., RAG, clustering, fine-tuning), and how it integrates with existing workflows.&gt;</p>"},{"location":"portfolio/projects/project_template/#solution-architecture","title":"\u2192 Solution Architecture","text":"<p>Short caption describing the architecture or link to a higher-resolution diagram.</p>"},{"location":"portfolio/projects/project_template/#tech-stack","title":"\u2192 Tech Stack","text":"<ul> <li>Infrastructure:  <li>Data Platform:  <li>CI/CD:  <li>Containerization:  <li>Backend:  <li>Modeling:  <li>Vector Index:"},{"location":"portfolio/projects/project_template/#key-learnings","title":"Key Learnings","text":"<ul> <li>:  <li>:  <li>:  <li>:"},{"location":"portfolio/projects/project_template/#measurable-impact","title":"Measurable Impact","text":"<ul> <li> <li> <li> <li> <ul> <li> <p> Let's have a virtual coffee together!</p> <p>Want to see if we're a match? Let's have a chat and find out. Schedule a free 30-minute strategy session to discuss your AI challenges and explore how we can work together.</p> <p>Book Intro Call </p> </li> </ul>"},{"location":"portfolio/projects/project_voice/","title":"Project Writing Voice","text":""},{"location":"portfolio/projects/project_voice/#tone-and-pov","title":"Tone and POV","text":"<ul> <li>Write in first person with clear ownership (e.g., \"I built\", \"I implemented\").</li> <li>Keep the tone confident, professional, and impact-focused.</li> <li>Prioritize clarity over hype; avoid fluff and vague claims.</li> <li>Emphasize business value alongside technical detail.</li> </ul>"},{"location":"portfolio/projects/project_voice/#structure","title":"Structure","text":"<ul> <li>Frontmatter includes <code>title</code> and a one-line <code>description</code> of impact.</li> <li>Use a single H1 that matches the title.</li> <li>Start with a <code>!!! abstract</code> case study summary containing Client, Industry, Role, and Impact Metrics.</li> <li>Include a concise one-paragraph overview after the abstract.</li> <li>Use sections in this order: Challenge \u2192 Solution \u2192 Key Learnings \u2192 Measurable Impact.</li> <li>Under Solution, include <code>\u2192 Implementation</code>, <code>\u2192 Solution Architecture</code>, and <code>\u2192 Tech Stack</code>.</li> <li>End with the virtual coffee CTA card block.</li> </ul>"},{"location":"portfolio/projects/project_voice/#formatting-conventions","title":"Formatting Conventions","text":"<ul> <li>Use sentence case headings.</li> <li>Use hyphen lists for metrics and tech stack entries.</li> <li>In the abstract block, use two spaces for line breaks after Client/Industry/Role.</li> <li>Use an HTML image tag for architecture diagrams with width set to 600.</li> <li>Add a short italicized caption below the diagram.</li> <li>Use bold labels with colons in Key Learnings bullets (e.g., \"Scalability: ...\").</li> <li>Keep paragraphs short, typically 2\u20134 sentences.</li> </ul>"},{"location":"portfolio/projects/project_voice/#metrics-and-evidence","title":"Metrics and Evidence","text":"<ul> <li>Lead with measurable results (time reduction, latency, cost savings, adoption).</li> <li>Prefer concrete comparisons (e.g., \"5 days \u2192 0.5 day\", \"20 \u2192 300 users\").</li> <li>Include both operational impact and strategic outcomes.</li> <li>Avoid unquantified superlatives unless necessary.</li> </ul>"},{"location":"portfolio/projects/project_voice/#technical-detail","title":"Technical Detail","text":"<ul> <li>Name key components, frameworks, and infrastructure explicitly.</li> <li>Describe pipelines as staged flows (e.g., \"ingestion \u2192 processing \u2192 modeling \u2192 serving\").</li> <li>Mention privacy, security, or compliance decisions where relevant.</li> <li>Call out integration points and data sources when meaningful.</li> </ul>"},{"location":"youtube/","title":"YouTube Channel","text":"<p>Redirecting to Miguel Miranda Dias YouTube Channel in a new window...</p> <p>If you're not redirected automatically, click here.</p>"},{"location":"blog/archive/2026/","title":"2026","text":""},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/tools/","title":"Tools","text":""},{"location":"blog/category/development/","title":"Development","text":""},{"location":"blog/category/agents/","title":"Agents","text":""},{"location":"blog/category/python/","title":"Python","text":""},{"location":"blog/category/data-science/","title":"Data Science","text":""}]}