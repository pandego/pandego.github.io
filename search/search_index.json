{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About Me","text":""},{"location":"#hi-im-miguel-i-build-ai-systems-and-document-the-journey","title":"Hi, I'm Miguel \ud83d\udc4b\ud83c\udffc I build AI systems and document the journey","text":""},{"location":"#a-10-years-journey-turning-ai-theory-into-production-reality","title":"A 10+ years journey turning AI theory into production reality","text":"<ul> <li>Wondering how AI solves YOUR specific business problems?</li> <li>Tired of AI projects that never make it past the PowerPoint?</li> <li>Overwhelmed sorting real AI solutions from the noise?</li> <li>Worried about hallucinations, privacy, and vendor lock-in?</li> <li>Need a clear roadmap from AI pilot to production?</li> <li>Ready to be in the 5%<sup>1</sup> whose AI solutions actually bring ROI?</li> </ul> <p>Book Intro Call </p>"},{"location":"#about-me","title":"About me","text":"<p>My mission is simple \u2192 Protect startups and teams from buying AI they can't use.</p> <p>After a decade building AI systems across medtech, automotive, and enterprise sectors, I've seen too many companies burned by consultancies selling PowerPoints instead of working code. Technical teams get sidelined, projects fail, and AI becomes another expensive disappointment. I founded Datavengers to change that equation.</p> <p>My path here wasn't typical. I started in research (PhD in genomics, postdoc in gene therapy) before recognizing that AI would reshape every industry. So I pivoted! Earned my Master's in Data Science &amp; Artificial Intelligence and dove into industry, where I ship production-grade AI systems that actually deliver ROI. My academic foundation gave me the rigor, but it's the years of shipping production code that taught me what actually works. From patent analysis systems processing thousands of documents to computer vision catching defects on production lines, I've learned what works beyond the proof-of-concept stage. This unique background means I can translate between data scientists writing the code, executives signing the checks, and users living with the results.</p> <p>Today, through Datavengers and my educational content, I help companies implement AI that sticks and developers avoid the mistakes I've made. Because the real challenge isn't understanding AI - it's making it work reliably, at scale, with real data and real constraints. That's where I come in.</p>"},{"location":"#what-people-say-about-working-with-me","title":"What people say about working with me","text":"<ul> <li> <p> Ludovic Gardy</p> <p>Founder at Sotis Advanced Insights</p> <p>\"Miguel stands out for his ability to quickly grasp complex topics, understand critical challenges, and identify key issues. His expertise in AI and MLOps enables him to build robust pipelines and contribute effectively to end-to-end solutions, always delivering clean, efficient execution with a proactive mindset. Importantly, he excels in an industrial environment by skillfully managing project constraints and task coordination while developing comprehensive roadmaps with clearly defined scopes.\"</p> </li> <li> <p> Nicolas Baillot d'Estivaux</p> <p>Chief Data Officer at bioM\u00e9rieux</p> <p>\"Miguel has a very strong technical background, he's able to lead important projects, to make the accurate technical choices and always learns new things and try to implement them in the profesional context. His support was highly valuable for the data team and for the company.\"</p> </li> <li> <p> Mehdi Elion</p> <p>Senior Data Scientist at Mirakle</p> <p>\"Miguel has definitely shown the best qualities one could dream of in an AI Tech Lead. He's kind, passionate, supportive, encourages you to grow both professionally and technically. He's also one of the most reliable techies I've worked with: Miguel is the go-to guy whether it is for pure data science, hardware setup, solution deployment or even networking issues. It has been an honor to work with him, and I strongly recommend to have him in your team if you get the opportunity to do so.\"</p> </li> <li> <p> Antonio Jacinto</p> <p>Plant Manager at Forvia</p> <p>\"Miguel's expertise in machine learning and deep learning, particularly in computer vision and its application to industrial cases has been pivotal in our success. His programming skills are exceptional, and his ability to navigate complex technical challenges and to propose innovative solutions significantly contributed to the success of our initiatives. His contributions have left a lasting impact.\"</p> </li> </ul>"},{"location":"#frequently-asked-questions","title":"Frequently asked questions","text":"How quickly can you start working on my project? <p>I can typically begin new projects within 1-2 weeks of contract signing. For urgent matters, I maintain some flexibility for rapid response situations and can potentially start sooner - just let me know your timeline during our initial consultation.</p> Do you require a minimum project size or commitment? <p>While I can accommodate projects of any size, I find that engagements of at least 20 hours per week allow for meaningful impact. This gives us enough time to understand your data, implement solutions, and deliver actionable results. We can start with a small pilot project to ensure we're a good fit.</p> What industries do you have experience in? <p>I've successfully delivered projects across biotech, healthcare, manufacturing, and automotive sectors. While I specialize in data science &amp; AI fundamentals that apply across sectors, I particularly excel in projects involving the implementation of AI solutions such as RAG and LLM integration for enterprise applications.</p> How do you handle data security and confidentiality? <p>I take data security extremely seriously. I sign comprehensive NDAs before starting any project, use enterprise-grade encryption for all data transfers, and follow industry best practices for data handling. I can also work within your existing security infrastructure and policies.</p> What's your pricing structure? <p>I offer both project-based and retainer pricing models. Project fees are based on scope, complexity, and value delivered rather than hours worked. For ongoing support, I offer flexible retainer packages. Let's discuss your specific needs during our consultation to determine the most cost-effective approach.</p> How do you communicate progress and results? <p>I maintain clear communication through weekly progress updates and regular check-in meetings. You'll receive detailed documentation of all analyses, findings, and recommendations. For ongoing projects, I provide interactive dashboards and reports that allow you to track progress and results in real-time.</p> <ul> <li> <p> Let's have a virtual coffee together!</p> <p>Want to see if we're a match? Let's have a chat and find out. Schedule a free 30-minute strategy session to discuss your AI challenges and explore how we can work together.</p> <p>Book Intro Call </p> </li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/","title":"From Skeptic to Believer: Unpacking the Model Context Protocol","text":"<p>I\u2019ll admit it , when I first heard about the Model Context Protocol (MCP), my eyes nearly rolled out of my head. Having worn the data scientist hat for over a decade, I\u2019ve watched plenty of hyped frameworks come and go. But curiosity got the better of me. I dug in, tried it out, and well, I was pleasantly surprised. It turns out MCP actually is useful!</p> <p>Condensed mini\u2011blog from my piece on the Model Context Protocol (MCP). Read full article on Medium.</p>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#mcp-in-one-breath","title":"MCP in one breath","text":"<p>It\u2019s a protocol, not a framework. Think USB\u2011C for LLM apps.</p> <ul> <li>Host: your LLM application (chatbot, editor plugin, desktop app). The decider.</li> <li>Client: the in\u2011app component that speaks MCP to servers. The messenger.</li> <li>Server: exposes Resources (read\u2011only data), Tools (actions), and Prompts (reusable templates). The do\u2011er.</li> </ul> <p>One server can power many apps; one app can connect to many servers. No model/vendor lock\u2011in.</p>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#the-three-primitives-with-bitesized-code","title":"The three primitives (with bite\u2011sized code)","text":""},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#1-resources-get-me-context","title":"1) Resources \u2014 \"GET me context\"","text":"<p>Read\u2011only handles to data you want the model to see.</p> <pre><code>from mcp.server.fastmcp import FastMCP\nmcp = FastMCP(\"My App\")\n\n@mcp.resource(\"config://app\")\ndef get_config() -&gt; str:\n    return \"App configuration here\"\n\n@mcp.resource(\"users://{user_id}/profile\")\ndef get_user_profile(user_id: str) -&gt; str:\n    return f\"Profile data for user {user_id}\"\n</code></pre> <p>Use for configs, user metadata, document contents, preloaded business context.</p>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#2-tools-do-the-thing","title":"2) Tools \u2014 \"Do the thing\"","text":"<p>Side\u2011effectful actions or computations (sync/async).</p> <pre><code>import httpx\nfrom mcp.server.fastmcp import FastMCP\nmcp = FastMCP(\"My App\")\n\n@mcp.tool()\ndef calculate_bmi(weight_kg: float, height_m: float) -&gt; float:\n    return weight_kg / (height_m ** 2)\n\n@mcp.tool()\nasync def fetch_weather(city: str) -&gt; str:\n    async with httpx.AsyncClient() as client:\n        r = await client.get(f\"https://api.weather.com/{city}\")\n        return r.text\n</code></pre> <p>Great for API calls, business logic, CRUD, automation.</p>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#3-prompts-stop-rewriting-templates","title":"3) Prompts \u2014 \"Stop rewriting templates\"","text":"<p>Server\u2011defined, reusable prompt patterns.</p> <pre><code>from mcp.server.fastmcp import FastMCP\nfrom mcp.server.fastmcp.prompts import base\nmcp = FastMCP(\"My App\")\n\n@mcp.prompt()\ndef review_code(code: str) -&gt; str:\n    return f\"Please review this code:\\n\\n{code}\"\n\n@mcp.prompt()\ndef debug_error(error: str) -&gt; list[base.Message]:\n    return [\n        base.UserMessage(\"I'm seeing this error:\"),\n        base.UserMessage(error),\n        base.AssistantMessage(\"I'll help debug that. What have you tried so far?\"),\n    ]\n</code></pre> <p>Handy for code review, support workflows, query templates, standardized outputs.</p>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#a-practical-build-wrap-an-mlflow-model-with-mcp","title":"A practical build: wrap an MLflow model with MCP","text":"<p>Goal: expose a wine\u2011quality predictor (served via MLflow) to any MCP\u2011compatible host (e.g., Claude Desktop).</p> <p>Tool: <code>predict_wine_quality</code></p> <pre><code># server.py\n@mcp.tool(name=\"predict_wine_quality\", description=\"Predict wine quality using MLflow API\")\nasync def predict_wine_quality(inputs: list[list[float]], columns: list[str]) -&gt; list[float]:\n    payload = {\"dataframe_split\": {\"data\": inputs, \"columns\": columns}}\n    async with httpx.AsyncClient() as client:\n        resp = await client.post(MLFLOW_URL, json=payload)\n    return resp.json()[\"predictions\"]\n</code></pre> <p>Resource: example payload to guide users</p> <pre><code>@mcp.resource(\n    uri=\"wine://example\",\n    name=\"wine_quality_example\",\n    description=\"Example wine quality inputs and outputs\",\n    mime_type=\"application/json\",\n)\ndef get_input_example() -&gt; str:\n    import json\n    return json.dumps({\n        \"columns\": [\n            \"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\",\n            \"chlorides\", \"free sulfur dioxide\", \"total sulfur dioxide\",\n            \"density\", \"pH\", \"sulphates\", \"alcohol\"\n        ],\n        \"data\": [\n            [7.4, 0.7, 0, 1.9, 0.076, 11, 34, 0.9978, 3.51, 0.56, 9.4],\n            [7.8, 0.88, 0, 2.6, 0.098, 25, 67, 0.9968, 3.2, 0.68, 9.8]\n        ]\n    }, indent=2)\n</code></pre> <p>Prompt: tidy the output for chat</p> <pre><code>@mcp.prompt(name=\"format_predictions\", description=\"Format wine quality predictions for chatbot\")\ndef format_predictions(predictions: list[float]) -&gt; str:\n    formatted = \"\\n\".join(f\"- Sample {i+1}: **{s:.2f}/10**\" for i, s in enumerate(predictions))\n    return f\"## Predicted Wine Quality Scores\\n\\n{formatted}\"\n</code></pre>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#dev-ux-test-containerize-connect","title":"Dev UX: test, containerize, connect","text":"<p>Local dev &amp; inspector</p> <pre><code>mcp dev src/mcp_server_mlflow/server.py\n</code></pre> <p>Containerize it (recommended)</p> <pre><code>FROM python:3.11-slim-bookworm\nWORKDIR /app\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\n    build-essential curl git &amp;&amp; rm -rf /var/lib/apt/lists/*\nCOPY pyproject.toml uv.lock README.md ./\nCOPY src/ ./src/\nRUN pip install uv &amp;&amp; uv venv &amp;&amp; uv sync\nRUN useradd -ms /bin/bash appuser\nUSER appuser\nENV PATH=\"/app/.venv/bin:$PATH\"\nENV MLFLOW_URL=\"http://host.docker.internal:1234/invocations\"\nENTRYPOINT [\"mcp-server-mlflow\"]\n</code></pre> <p>Build it:</p> <pre><code>docker build -t mcp/wine .\n</code></pre> <p>Wire it to Claude Desktop (or any MCP host)</p> <p>Option A \u2014 Docker:</p> <pre><code>{\n  \"mcpServers\": {\n    \"My Wine Quality Server (docker)\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"--init\", \"mcp/wine\"]\n    }\n  }\n}\n</code></pre> <p>Option B \u2014 <code>uv</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"My Wine Quality Server (uv)\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"/PATH/TO/PROJECT\", \"run\", \"mcp-server-mlflow\"]\n    }\n  }\n}\n</code></pre>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#when-mcp-shines-and-when-its-overkill","title":"When MCP shines (and when it\u2019s overkill)","text":"<p>Use MCP when:</p> <ul> <li>You have multiple apps/agents that need the same tools/data/prompts.</li> <li>You want model\u2011agnostic integrations (swap providers without rewiring).</li> <li>Teams should share capabilities via a common, versionable interface.</li> </ul> <p>Skip MCP when:</p> <ul> <li>It\u2019s a one\u2011off script with a single hardcoded API call.</li> <li>There\u2019s no reuse across apps or teammates.</li> </ul>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#takeaways","title":"Takeaways","text":"<ul> <li>MCP standardizes how LLM apps access resources, tools, and prompts.</li> <li>It trades brittle per\u2011app glue for composable, reusable, portable servers.</li> <li>The Python SDK + <code>mcp dev</code> + Docker make it straightforward to ship.</li> </ul>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#wrapup","title":"Wrap\u2011up","text":"<p>I started a skeptic. After wiring a real MLflow model through MCP and dropping it into a chat host, I\u2019m\u2026 converted. If your LLM features are stuck in \u201cautocomplete\u201d mode, MCP is the cleanest path I\u2019ve seen to make them world\u2011aware without bespoke spaghetti.</p>"},{"location":"blog/2025/07/03/from-skeptic-to-believer-unpacking-the-model-context-protocol/#read-the-full-article","title":"\ud83d\udcd6 Read the Full Article","text":"From Skeptic to Believer: Unpacking the Model Context Protocol <p>MCP is a powerful open protocol that lets LLM apps connect to tools and data sources in a standardized, plug-and-play way\u2014kind of like REST for AI.</p>          \ud83d\udcd6 Full article available on Medium"},{"location":"blog/2023/03/05/red-wine-and-dockerized-mlflow-a-love-story-between-devops-and-data-science/","title":"Red Wine and Dockerized MLflow: A Love Story Between DevOps and Data Science","text":"<p>Are you tired of messy machine learning pipelines? Do you struggle to keep track of your experiments and their results? Fear not, because MLflow and Docker Compose are here to save the day!</p> <p>Condensed mini\u2011blog from my piece on self-hosting an MLflow instance. Read full article on Medium.</p>"},{"location":"blog/2023/03/05/red-wine-and-dockerized-mlflow-a-love-story-between-devops-and-data-science/#why-this-stack-works","title":"Why this stack works","text":"<p>If you\u2019re wrangling ML experiments, MLflow is your control tower. Pair it with PostgreSQL for the tracking backend and MinIO (S3\u2011compatible) for artifact storage, then wrap the whole thing in Docker Compose so it\u2019s reproducible and easy to boot up anywhere. Result: clean experiment tracking, model registry, and artifact management without yak\u2011shaving.</p> <p>Ingredients</p> <ul> <li>MLflow: experiment tracking, model registry, model serving</li> <li>Postgres: durable metadata (experiments, runs, metrics)</li> <li>MinIO: object store for artifacts/models (S3 API)</li> <li>Docker Compose: one command to run them all</li> </ul>"},{"location":"blog/2023/03/05/red-wine-and-dockerized-mlflow-a-love-story-between-devops-and-data-science/#prereqs","title":"Prereqs","text":"<ul> <li>Ubuntu 20.04 LTS (WSL2 on Windows works too)</li> <li>Miniconda, Docker, Docker Compose</li> <li>Optional but handy: pyenv to keep Python versions isolated</li> </ul> <pre><code># Install pyenv (Ubuntu)\nsudo apt-get update -y &amp;&amp; \\\n  sudo apt-get install -y make build-essential libssl-dev zlib1g-dev \\\n  libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm \\\n  libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev \\\n  libffi-dev liblzma-dev\ncurl https://pyenv.run | bash\n# Add to ~/.bashrc\nexport PYENV_ROOT=\"$HOME/.pyenv\"\nexport PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init --path)\"\nsource ~/.bashrc\n</code></pre> <p>Create a working env and set S3/MLflow env vars:</p> <pre><code>conda create -n mlflow_env python=3.11 -y\nconda activate mlflow_env\npip install pandas scikit-learn mlflow[extras]\n\nexport AWS_ACCESS_KEY_ID=minio\nexport AWS_SECRET_ACCESS_KEY=minio123   # change in .env\nexport MLFLOW_S3_ENDPOINT_URL=http://localhost:9000\n</code></pre>"},{"location":"blog/2023/03/05/red-wine-and-dockerized-mlflow-a-love-story-between-devops-and-data-science/#compose-it-all-together","title":"Compose it all together","text":"<p>Clone the template project:</p> <pre><code>git clone https://github.com/pandego/mlflow-postgres-minio.git\ncd mlflow-postgres-minio\ncp default.env .env   # edit secrets/ports as needed\n</code></pre> <p>Key services (trimmed):</p> <pre><code>services:\n  db:\n    image: postgres:${PG_VERSION}\n    environment:\n      - POSTGRES_USER=${PG_USER}\n      - POSTGRES_PASSWORD=${PG_PASSWORD}\n      - POSTGRES_DATABASE=${PG_DATABASE}\n    healthcheck:\n      test: [\"CMD\", \"pg_isready\", \"-p\", \"${PG_PORT_SERVICE}\", \"-U\", \"${PG_USER}\"]\n\n  s3:\n    image: minio/minio:${MINIO_VERSION}\n    command: server /data --console-address \":9001\" --address \":9000\"\n    environment:\n      - MINIO_ROOT_USER=${MINIO_ROOT_USER}\n      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}\n\n  create_buckets:\n    image: minio/mc:${MINIO_VERSION}\n    depends_on: { s3: { condition: service_healthy } }\n    entrypoint: &gt;\n      /bin/sh -c '\n      sleep 5;\n      mc config host add s3 http://s3:${MINIO_PORT_API} ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD} --api S3v4;\n      mc mb s3/${MLFLOW_BUCKET_NAME} || true;\n      mc policy download s3/${MLFLOW_BUCKET_NAME};\n      mc mb s3/${DATA_REPO_BUCKET_NAME} || true;\n      mc policy download s3/${DATA_REPO_BUCKET_NAME};\n      '\n\n  tracking_server:\n    build: ./mlflow\n    command: &gt;\n      mlflow server \\\n        --backend-store-uri postgresql://${PG_USER}:${PG_PASSWORD}@db:${PG_PORT_SERVICE}/${PG_DATABASE} \\\n        --host 0.0.0.0 \\\n        --port ${MLFLOW_PORT_SERVICE} \\\n        --default-artifact-root s3://mlflow/\n    environment:\n      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}\n      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}\n      - MLFLOW_S3_ENDPOINT_URL=http://s3:${MINIO_PORT_API}\n</code></pre> <p>Bring it up:</p> <pre><code>docker-compose --env-file .env up -d --build\ndocker ps -a   # all services should be healthy\n</code></pre> <p>UIs</p> <ul> <li>MLflow Tracking: http://localhost:5000</li> <li>MinIO Console: http://localhost:9001 (API is :9000)</li> </ul>"},{"location":"blog/2023/03/05/red-wine-and-dockerized-mlflow-a-love-story-between-devops-and-data-science/#train-a-model-elasticnet-on-red-wine","title":"Train a model (ElasticNet on red wine)","text":"<p>Inside <code>./wine_quality_example/</code> you\u2019ll find <code>wine_quality_data.csv</code> and <code>train.py</code>. The script trains a scikit\u2011learn ElasticNet regressor to predict wine quality and logs everything to MLflow.</p> <pre><code># metrics helper\ndef eval_metrics(y_true, y_pred):\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    mae = mean_absolute_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n    return rmse, mae, r2\n\n# point MLflow at your server\nmlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"red-wine-elasticnet\")\n\nwith mlflow.start_run():\n    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n    model.fit(train_x, train_y)\n    preds = model.predict(test_x)\n    rmse, mae, r2 = eval_metrics(test_y, preds)\n\n    mlflow.log_param(\"alpha\", alpha)\n    mlflow.log_param(\"l1_ratio\", l1_ratio)\n    mlflow.log_metric(\"rmse\", rmse)\n    mlflow.log_metric(\"mae\", mae)\n    mlflow.log_metric(\"r2\", r2)\n    mlflow.sklearn.log_model(model, \"model\")\n</code></pre> <p>Run it:</p> <pre><code>cd wine_quality_example\npython train.py\n</code></pre> <p>You should see a new experiment/run in the MLflow UI with params, metrics, and a logged model artifact stored in MinIO.</p>"},{"location":"blog/2023/03/05/red-wine-and-dockerized-mlflow-a-love-story-between-devops-and-data-science/#serve-the-model","title":"Serve the model","text":"<p>Serve any logged run locally with MLflow\u2019s lightweight server:</p> <pre><code>mlflow models serve \\\n  -m s3://mlflow/1/&lt;run_id&gt;/artifacts/model \\\n  -p 1234 --timeout 0\n</code></pre> <p>Smoke\u2011test with <code>curl</code> (JSON <code>dataframe_split</code>):</p> <pre><code>curl -X POST \\\n  -H \"Content-Type: application/json\" \\\n  --data '{\n    \"dataframe_split\": {\n      \"data\": [[7.4,0.7,0,1.9,0.076,11,34,0.9978,3.51,0.56,9.4]],\n      \"columns\": [\n        \"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\n        \"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\n        \"pH\",\"sulphates\",\"alcohol\"\n      ]\n    }\n  }' http://127.0.0.1:1234/invocations\n# \u2192 {\"predictions\": [5.576883967129616]}\n</code></pre>"},{"location":"blog/2023/03/05/red-wine-and-dockerized-mlflow-a-love-story-between-devops-and-data-science/#tips-gotchas","title":"Tips &amp; gotchas","text":"<ul> <li>Keep <code>.env</code> in sync with your conda env variables (keys/ports/bucket names).</li> <li>MinIO exposes API on :9000 and Console on :9001 by default\u2014both must be reachable from the tracking server.</li> <li>Healthchecks in Compose catch misconfig early; don\u2019t disable them.</li> <li>For production, put Postgres/MinIO on persistent volumes you back up.</li> </ul>"},{"location":"blog/2023/03/05/red-wine-and-dockerized-mlflow-a-love-story-between-devops-and-data-science/#wrapup","title":"Wrap\u2011up","text":"<p>MLflow + Postgres + MinIO, dockerized with Compose, gives you a reproducible, portable MLOps base in minutes. Log parameters and metrics, register and serve models, and keep artifacts tidy\u2014all while avoiding snowflake setups. Pour yourself a glass; your experiments are finally organized.</p>"},{"location":"blog/2023/03/05/red-wine-and-dockerized-mlflow-a-love-story-between-devops-and-data-science/#read-the-full-article","title":"\ud83d\udcd6 Read the Full Article","text":"Red Wine, ElasticNet, and Dockerized MLflow with Postgres and MinIO: A Love Story Between DevOps and Data Science <p>A comprehensive guide on setting up MLflow with Docker, using Postgres for tracking and MinIO for artifact storage, demonstrated through a wine quality prediction model.</p>          \ud83d\udcd6 Full article available on Medium"},{"location":"blog/2023/03/05/no-openai-account-no-problem/","title":"No OpenAI Account, No Problem!","text":"<p>If you want the OpenAI developer experience without the OpenAI account (or tokens), Ollama now exposes an OpenAI-style <code>/v1</code> endpoint. That means you can point existing clients and frameworks at your local models and ship.</p> <p>Condensed mini-blog from my piece on crafting your own OpenAI-compatible API with Ollama. Read full article on Medium.</p>"},{"location":"blog/2023/03/05/no-openai-account-no-problem/#why-this-rocks","title":"Why this rocks","text":"<ul> <li>Drop\u2011in compatibility: Keep using the OpenAI SDKs and patterns.</li> <li>Local first: Your data and prompts stay on your machine.</li> <li>Costs: $0 per token, just your hardware.</li> </ul>"},{"location":"blog/2023/03/05/no-openai-account-no-problem/#1-spin-up-ollama-with-docker","title":"1) Spin up Ollama with Docker","text":"<p>CPU only</p> <pre><code>docker run -d -v /data/ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n</code></pre> <p>GPU (NVIDIA)</p> <ol> <li>Install NVIDIA Container Toolkit.</li> <li>Run with GPU access:</li> </ol> <pre><code>docker run -d --gpus=all -v /data/ollama:/root/.ollama --restart always -p 11434:11434 --name ollama ollama/ollama\n</code></pre> <p>Pro tip: Want specific GPUs? Use <code>--gpus \"device=0,1\"</code>.</p> <p>Where are models stored? In this compose, they\u2019ll live on your host at <code>/data/ollama</code>.</p>"},{"location":"blog/2023/03/05/no-openai-account-no-problem/#2-sanitycheck-ollama","title":"2) Sanity\u2011check Ollama","text":"<p>Run a model inside the container:</p> <pre><code>docker exec -it ollama ollama run llama2\n# &gt;&gt;&gt; Send a message (/? for help)\n</code></pre> <p>Hit the OpenAI\u2011style chat completions endpoint:</p> <pre><code>curl http://localhost:11434/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"model\": \"llama2\",\n        \"messages\": [\n          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n          {\"role\": \"user\",   \"content\": \"Hello!\"}\n        ]\n      }'\n</code></pre> <p>If you get a response with <code>choices[0].message.content</code>, you\u2019re golden.</p>"},{"location":"blog/2023/03/05/no-openai-account-no-problem/#3-build-a-local-openai-chatbot","title":"3) Build a local \u201cOpenAI\u201d chatbot","text":"<p>Requirements</p> <pre><code>streamlit&gt;=1.28\nlangchain&gt;=0.0.217\nopenai&gt;=1.2\nduckduckgo-search\nanthropic&gt;=0.3.0\ntrubrics&gt;=1.4.3\nstreamlit-feedback\n</code></pre> <p>Minimal app: <code>Chatbot.py</code></p> <pre><code>from openai import OpenAI\nimport streamlit as st\n\n# This key is required by the SDK but unused by Ollama; leave any string\nOPENAI_API_KEY = \"ollama-baby\"\n\nst.title(\"Chatbot\")\nst.caption(\"A Streamlit chatbot powered by OpenAI API... I mean Ollama!!!\")\n\nif \"messages\" not in st.session_state:\n    st.session_state[\"messages\"] = [\n        {\"role\": \"assistant\", \"content\": \"How can I help you?\"}\n    ]\n\n# Show history\nfor msg in st.session_state[\"messages\"]:\n    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n\n# Input\nprompt = st.chat_input(\"Say something\u2026\")\nif prompt:\n    client = OpenAI(\n        api_key=OPENAI_API_KEY,\n        base_url=\"http://localhost:11434/v1\",  # \u2190 point at Ollama\n    )\n\n    st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": prompt})\n    st.chat_message(\"user\").write(prompt)\n\n    response = client.chat.completions.create(\n        model=\"llama2\",\n        messages=st.session_state[\"messages\"],\n    )\n    msg = response.choices[0].message.content\n\n    st.session_state[\"messages\"].append({\"role\": \"assistant\", \"content\": msg})\n    st.chat_message(\"assistant\").write(msg)\n</code></pre> <p>Run it:</p> <pre><code>streamlit run Chatbot.py\n</code></pre> <p> Still not a standup comedian \ud83d\ude2c</p>"},{"location":"blog/2023/03/05/no-openai-account-no-problem/#notes","title":"Notes","text":"<ul> <li>You can swap <code>llama2</code> for any local model you\u2019ve pulled with Ollama.</li> <li>Keep an eye on VRAM/CPU footprints; bigger models need beefier hardware.</li> <li>LangChain, agents, and retrieval components can ride along since the client looks like OpenAI.</li> </ul>"},{"location":"blog/2023/03/05/no-openai-account-no-problem/#wrapup","title":"Wrap\u2011up","text":"<p>With Ollama\u2019s <code>/v1</code> endpoint, you can prototype and ship OpenAI\u2011compatible apps locally. The DX you know, the privacy you want, and zero token anxiety.</p>"},{"location":"blog/2023/03/05/no-openai-account-no-problem/#read-the-full-article","title":"\ud83d\udcd6 Read the Full Article","text":"No OpenAI Account, No Problem! Crafting Your Own OpenAI API with Ollama \ud83e\udd99 <p>Explore alternatives to OpenAI's services and learn how to leverage open-source models for your AI projects.</p>          \ud83d\udcd6 Full article available on Medium"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/","title":"Predicting the Unpredictable: The Magic of Mixture Density Networks Explained","text":"<p>Tired of your neural networks making lame predictions? \ud83e\udd26\u200d\u2642\ufe0f Wish they could predict more than just the average future? Enter Mixture Density Networks (MDNs), a supercharged approach that doesn\u2019t just guess the future \u2014 it predicts a whole spectrum of possibilities!</p> <p>Condensed mini\u2011blog from my piece on Mixture Density Networks for uncertainty-aware regression. Read full article on Medium.</p>"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/#how-an-mdn-works-in-one-gulp","title":"How an MDN Works (in one gulp)","text":"<p>Given input \\(x\\), the network outputs:</p> <ul> <li>Mixture weights \\(\\alpha_k(x)\\), via softmax so they sum to 1</li> <li>Means \\(\\mu_k(x)\\)</li> <li>Standard deviations \\(\\sigma_k(x)\\), via exp to keep them positive</li> </ul> <p>Then the conditional density is:</p> <p>\\(p(t\\mid x) = \\sum_{k=1}^{K} \\alpha_k(x)\\, \\mathcal{N}\\big(t\\;\\big|\\;\\mu_k(x),\\, \\sigma_k^2(x)\\big).\\)</p>"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/#training-maximize-likelihood","title":"Training = Maximize Likelihood","text":"<p>We minimize negative log-likelihood (NLL) over the dataset \\(\\{(x_i, t_i)\\}\\):</p> <p>\\(\\mathcal{L} = - \\sum_i \\log\\Big[\\sum_k \\alpha_k(x_i)\\, \\mathcal{N}\\big(t_i\\;\\big|\\;\\mu_k(x_i),\\, \\sigma_k^2(x_i)\\big)\\Big].\\)</p> <p>This pushes the right components to \u201cown\u201d the right regions while learning both where mass should live (\\(\\mu\\)) and how uncertain it is (\\(\\sigma\\)).</p>"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/#a-compact-pytorch-mdn","title":"A Compact PyTorch MDN","text":"<p>Below is a tidy version of the loss and head you can drop into a regressor. (See the original for a full training loop and dataset plumbing.)</p> <pre><code># Loss for a Gaussian Mixture output\n# alpha: (N, K), sigma: (N, K, T), mu: (N, K, T), target: (N, T)\nimport torch, torch.nn.functional as F\n\ndef mdn_loss(alpha, sigma, mu, target, eps=1e-8):\n    target = target.unsqueeze(1).expand_as(mu)          # (N, 1, T) -&gt; (N, K, T)\n    m = torch.distributions.Normal(loc=mu, scale=sigma) # component log-probs\n    log_prob = m.log_prob(target).sum(dim=2)            # (N, K)\n    log_alpha = torch.log(alpha + eps)                  # avoid log(0)\n    loss = -torch.logsumexp(log_alpha + log_prob, dim=1)\n    return loss.mean()\n</code></pre> <pre><code># Minimal MDN head\nimport torch.nn as nn\n\nclass MDN(nn.Module):\n    def __init__(self, in_dim, out_dim, hidden, K):\n        super().__init__()\n        self.backbone = nn.Sequential(\n            nn.Linear(in_dim, hidden), nn.Tanh(),\n            nn.Linear(hidden, hidden), nn.Tanh(),\n        )\n        self.z_alpha = nn.Linear(hidden, K)\n        self.z_sigma = nn.Linear(hidden, K * out_dim)\n        self.z_mu    = nn.Linear(hidden, K * out_dim)\n        self.K = K\n        self.out_dim = out_dim\n\n    def forward(self, x):\n        h = self.backbone(x)\n        alpha = F.softmax(self.z_alpha(h), dim=-1)\n        sigma = torch.exp(self.z_sigma(h)).view(-1, self.K, self.out_dim)\n        mu    = self.z_mu(h).view(-1, self.K, self.out_dim)\n        return alpha, sigma, mu\n</code></pre>"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/#sampling-predictions","title":"Sampling Predictions","text":"<p>Turn mixture params into concrete draws to visualize possible futures:</p> <pre><code>import itertools\n\ndef sample_mdn(alpha, sigma, mu, samples=10):\n    N, K, T = mu.shape\n    preds = torch.zeros(N, samples, T)\n    u = torch.rand(N, samples)\n    csum = alpha.cumsum(dim=1)\n    for i, j in itertools.product(range(N), range(samples)):\n        k = torch.searchsorted(csum[i], u[i, j]).item()\n        preds[i, j] = torch.normal(mu[i, k], sigma[i, k])\n    return preds  # (N, samples, T)\n</code></pre>"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/#quick-case-study-apparent-temperature","title":"Quick Case Study: \u201cApparent Temperature\u201d \ud83c\udf21\ufe0f","text":"<p>Train an MDN (e.g., two hidden tanh layers of width \\~50) on a simple weather dataset to predict apparent temperature. You\u2019ll get both accurate central tendencies and sensible spread. Typical diagnostics:</p> <ul> <li>R\u00b2 near 0.99 (with careful preprocessing)</li> <li>MAE \u2248 0.5 degrees</li> <li>Histograms and scatter plots show measured vs. sampled predictions aligning closely</li> </ul> \u00d7 <p>Pro tips: remove outliers, consider resampling, and tune \\(K\\) and hidden width. Larger \\(K\\) gives more expressivity but can make training trickier.</p>"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/#when-to-reach-for-mdns","title":"When to Reach for MDNs","text":"<ul> <li>Targets with multiple valid outcomes for the same input (multi\u2011modal)</li> <li>Aleatoric uncertainty that varies with \\(x\\) (heteroscedastic noise)</li> <li>You care about full predictive distributions (not just point estimates)</li> <li>Examples: motion forecasting, demand spikes, sensor fusion, finance tails, weather nowcasting</li> </ul>"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/#gotchas-good-habits","title":"Gotchas &amp; Good Habits","text":"<ul> <li>Stability: add small epsilons; clamp/log\u2011sum\u2011exp as above.</li> <li>Initialization: start with smaller \\(K\\); increase once training is stable.</li> <li>Evaluation: don\u2019t just check RMSE\u2014use NLL, CRPS, calibration curves, and coverage of prediction intervals.</li> <li>Inference: report means, modes, and quantiles from the mixture; visualize multiple samples.</li> </ul>"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/#wrapup","title":"Wrap\u2011Up","text":"<p>MDNs bolt a probability distribution to your neural net, turning point predictions into a palette of possibilities. If your target is messy, multi\u2011peaked, or just plain chaotic, MDNs are a pragmatic, PyTorch\u2011friendly way to model \u201cthe unpredictable\u201d\u2014and say how confident you are.</p>"},{"location":"blog/2024/05/19/predicting-the-unpredictable-the-magic-of-mixture-density-networks-explained/#read-the-full-article","title":"\ud83d\udcd6 Read the Full Article","text":"Predicting the Unpredictable:The Magic of Mixture Density Networks Explained <p>Deep dive into predictive modeling techniques and strategies for handling uncertain outcomes in data science.</p>          \ud83d\udcd6 Full article available on Medium"},{"location":"datavengers/","title":"About Datavengers\u2122","text":"<p>Datavengers SAS is a data, analytics, and artificial intelligence company founded by Dr Miguel Miranda Dias to help businesses integrate AI and data into their operations. Our services include AI strategy development, team training, tool selection, and custom AI integration.</p> <p>With a mission to accelerate AI adoption, Datavengers also provides educational resources, beginner-friendly courses, and a business accelerator for tech professionals. To learn more, visit our website.</p> <p>Visit website </p>"},{"location":"portfolio/","title":"Featured Projects","text":"<p>Welcome to my portfolio of data science and AI projects. Each project demonstrates my expertise in delivering impactful solutions to real-world business challenges.</p> <ul> <li> <p>AI Patent Trend Analysis for EV Batteries</p> <p>End-to-end AI pipeline for classifying, translating, clustering, and summarizing patent filings to surface EV-battery technology trends for IP and innovation teams.</p> </li> </ul>"},{"location":"portfolio/projects/project-1/","title":"AI Customer Care Bot for Dev X","text":"Portfolio Best Practices <p>This is a simplified example project. When creating your own portfolio:</p> <ul> <li>Include detailed technical challenges and how you solved them</li> <li>Add specific metrics and KPIs that demonstrate impact</li> <li>Show code snippets of interesting implementations</li> <li>Include architecture diagrams and system designs</li> <li>Document your decision-making process</li> <li>Highlight your specific contributions to the project</li> <li>Add visuals of the final product (if possible)</li> </ul> <p>Case Study Summary</p> <p>Client: Dev X Website: devx.com Industry: Software Development  </p> <p>Impact Metrics:</p> <ul> <li>90% reduction in customer service overhead (projected)</li> <li>100% accuracy on initial evaluation datasets</li> <li>&lt; 3 second response time for customer inquiries</li> <li>Successfully transitioned 12 CSRs to account management roles</li> <li>$240,000 annual cost savings in customer support operations</li> </ul> <p>Dev X aims to reduce its customer service overhead by 90% over the next three years through AI, enabling their staff to focus on more rewarding roles and build better relationships with clients.</p>"},{"location":"portfolio/projects/project-1/#challenge","title":"Challenge","text":"<p>Their strategy involved transitioning customer service representatives to more rewarding account manager roles to enhance client relationships. They needed an AI solution that could efficiently handle routine customer inquiries while integrating seamlessly with their existing workflows.</p>"},{"location":"portfolio/projects/project-1/#our-approach","title":"Our Approach","text":"<p>We developed an AI chatbot specifically for Dev X's internal use, designed to assist customer service representatives in quickly accessing information. The solution was seamlessly integrated within Slack, the platform already used by their team, allowing for minimal disruption to existing workflows.</p>"},{"location":"portfolio/projects/project-1/#results-impact","title":"Results &amp; Impact","text":"<ul> <li>Response time under 3 seconds</li> <li>100% accuracy on initial evaluation datasets</li> <li>Successful integration with existing Slack workflows</li> <li>Currently expanding knowledge base coverage</li> <li>Simple activation through Slack mentions</li> </ul>"},{"location":"portfolio/projects/project-1/#solution-overview","title":"Solution Overview","text":"<p>Baseline OpenAI end-to-end chat reference architecture</p>"},{"location":"portfolio/projects/project-1/#tech-stack","title":"Tech Stack","text":"<ul> <li>OpenAI</li> <li>Pinecone vector database</li> <li>Slack API integration</li> <li>Microsoft Azure cloud infrastructure</li> <li>Python backend services</li> <li>FastAPI for RESTful endpoints</li> <li>Docker containerization</li> <li>GitHub Actions for CI/CD pipeline</li> </ul>"},{"location":"portfolio/projects/project-1/#additional-context","title":"Additional Context","text":"<ul> <li>Timeline: 3 months</li> <li>Team Size: 2 people (AI Engineer and Data Engineer)</li> <li>Role: AI Engineer</li> <li>Close collaboration with customer service team</li> <li>Ongoing knowledge base expansion</li> <li>Future plans include implementing feedback mechanism</li> </ul> <ul> <li> <p> Let's have a virtual coffee together!</p> <p>Want to see if we're a match? Let's have a chat and find out. Schedule a free 30-minute strategy session to discuss your AI challenges and explore how we can work together.</p> <p>Book Intro Call </p> </li> </ul>"},{"location":"portfolio/projects/project-2/","title":"Enterprise Chatbot for the Company Y","text":"Portfolio Best Practices <p>This is a simplified example project. When creating your own portfolio:</p> <ul> <li>Include detailed technical challenges and how you solved them</li> <li>Add specific metrics and KPIs that demonstrate impact</li> <li>Show code snippets of interesting implementations</li> <li>Include architecture diagrams and system designs</li> <li>Document your decision-making process</li> <li>Highlight your specific contributions to the project</li> <li>Add visuals of the final product (if possible)</li> </ul> <p>Case Study Summary</p> <p>Client: Dev X Website: devx.com Industry: Software Development  </p> <p>Impact Metrics:</p> <ul> <li>90% reduction in customer service overhead (projected)</li> <li>100% accuracy on initial evaluation datasets</li> <li>&lt; 3 second response time for customer inquiries</li> <li>Successfully transitioned 12 CSRs to account management roles</li> <li>$240,000 annual cost savings in customer support operations</li> </ul> <p>Company Y an AI project featuring a private ChatGPT-like tool, streamlining mobility data analysis and advancing digital innovation in public sector policy evaluation.</p>"},{"location":"portfolio/projects/project-2/#challenge","title":"Challenge","text":"<p>The regional data team at Company Y faced the challenge of analyzing complex mobility data, including cars, bridges, traffic, and cyclists. Tasked with assessing policy compliance and the impact of changes, they struggled with data scattered across multiple systems, such as the Dexter portal's structured SQL data and various policy documents. This dispersion made analysis laborious, prompting the Province to explore how digitization and AI could streamline the process and foster innovation.</p>"},{"location":"portfolio/projects/project-2/#our-approach","title":"Our Approach","text":"<p>To tackle this challenge, we developed a custom-built AI solution similar to a \"private version of ChatGPT.\" This tool was designed to access and analyze large volumes of PDF documents and structured data exported from the Dexter database. By enabling a ChatGPT-like interaction, users could query this diverse data pool in a conversational manner, leveraging the AI to gain company-specific insights.</p>"},{"location":"portfolio/projects/project-2/#results-impact","title":"Results &amp; Impact","text":"<ul> <li>Successfully integrated structured SQL data and unstructured PDF documents</li> <li>Featured in major company meetings</li> <li>Enabled conversational querying of complex mobility data</li> <li>Streamlined policy compliance assessment</li> <li>Enhanced decision-making through comprehensive data analysis</li> </ul>"},{"location":"portfolio/projects/project-2/#solution-overview","title":"Solution Overview","text":"<p>Baseline OpenAI end-to-end chat reference architecture</p>"},{"location":"portfolio/projects/project-2/#tech-stack","title":"Tech Stack","text":"<ul> <li>OpenAI</li> <li>Pinecone vector database</li> <li>Microsoft Azure cloud infrastructure</li> <li>Python backend services</li> <li>FastAPI for RESTful endpoints</li> <li>Docker containerization</li> <li>GitHub Actions for CI/CD pipeline</li> </ul>"},{"location":"portfolio/projects/project-2/#additional-context","title":"Additional Context","text":"<ul> <li>Timeline: 3 months</li> <li>Team Size: 2 people</li> <li>Role: AI Engineer</li> <li>Expertise in custom chatbot development</li> <li>Specialization in retrieval-augmented generation</li> <li>Focus on OpenAI model integration</li> </ul> <ul> <li> <p> Let's have a virtual coffee together!</p> <p>Want to see if we're a match? Let's have a chat and find out. Schedule a free 30-minute strategy session to discuss your AI challenges and explore how we can work together.</p> <p>Book Free Intro Call </p> </li> </ul>"},{"location":"portfolio/projects/project-patent-trend-analytics/","title":"AI Patent Trend Analysis for EV Batteries (Confidential EU Enterprise)","text":"<p>Case Study Summary</p> <p>Client: Confidential (EU enterprise) Website: \u2014 Industry: Automotive (EV batteries)  </p> <p>Impact Metrics:</p> <ul> <li>\u221290% review/labeling time per weekly batch (from 5 days \u2192 0.5 day)  </li> <li>\u221290% manual classification overhead for the IP team  </li> <li>\u2248 \u20ac490k/year cost savings (10 IP engineers @ \u20ac50k/yr; weekly task reduced 5d \u2192 0.5d \u2192 45 person-days saved/week \u00d7 48 weeks \u00d7 \u20ac227/day)  </li> <li>Analysts reallocated from manual tagging to higher-value trend interpretation</li> </ul> <p>The IP team needed faster, consistent insight into thousands of new patents to inform R&amp;D on emerging EV-battery technologies. We delivered a pipeline that turns raw patent exports into titled clusters with summaries and a trends dashboard.</p>"},{"location":"portfolio/projects/project-patent-trend-analytics/#challenge","title":"Challenge","text":"<p>IP engineers were manually scanning and tagging multilingual patents\u2014slow, inconsistent, and hard to replicate at scale. Multilingual content and unstructured abstracts made it difficult to compare filings and report consolidated trends to innovation stakeholders.</p>"},{"location":"portfolio/projects/project-patent-trend-analytics/#our-approach","title":"Our Approach","text":"<p>We built REST endpoints for language detection \u2192 translation \u2192 domain-specific embedding \u2192 unsupervised clustering \u2192 cluster summarization \u2192 trend tracking, exposed via a FastAPI service on Azure and backed by a vector index for semantic lookups. Clusters receive concise titles/summaries and can optionally align to IPC categories for consistent reporting.</p>"},{"location":"portfolio/projects/project-patent-trend-analytics/#results-impact","title":"Results &amp; Impact","text":"<ul> <li>Weekly patent exports processed into digestible, titled clusters with auto-summaries  </li> <li>Review loops cut from hours to minutes per batch  </li> <li>Earlier visibility of emerging themes; analysts focus on interpretation vs manual tagging  </li> <li>Stable, repeatable releases with containerized CI/CD</li> </ul>"},{"location":"portfolio/projects/project-patent-trend-analytics/#solution-overview","title":"Solution Overview","text":"<p>Baseline EV-battery patent trend analytics solution architecture</p>"},{"location":"portfolio/projects/project-patent-trend-analytics/#tech-stack","title":"Tech Stack","text":"<ul> <li>Cloud: Microsoft Azure Cloud Infrastructure</li> <li>Data Platform: Azure Databricks (APIs for ingestion &amp; jobs)</li> <li>Vector Index: Databricks Vector Search</li> <li>Backend: Python services with FastAPI (REST)</li> <li>Containerization: Docker</li> <li>CI/CD: Azure DevOps Pipelines</li> <li>Language Detection: XLM-Roberta</li> <li>Translation: mBART-large-50 many to one</li> <li>Embeddings: BERT for Patents (fine-tuned for EV-battery patents)</li> <li>LLM (summarization): Mistral 7B</li> </ul>"},{"location":"portfolio/projects/project-patent-trend-analytics/#additional-context","title":"Additional Context","text":"<ul> <li>Timeline: ~4 months  </li> <li>Team Size: 4 people (2 Data Scientists, 1 MLOps Engineer, 1 Lead AI Engineer)</li> <li>Role: Lead AI Engineer  </li> <li>Collaboration: Close with IP analysts for evaluation loops and taxonomy alignment  </li> <li>Future Plans: Feedback signals into clustering/evals; optional supervised topic labels</li> </ul> <p>Three Key Points</p> <ol> <li>Domain-specific embeddings tuned for EV-battery patents to improve cluster cohesion.  </li> <li>Unsupervised clustering with optional IPC-guided labels for standardized taxonomy.  </li> <li>Trend analytics dashboard to monitor cluster momentum and surface emerging themes.</li> </ol> <ul> <li> <p> Let's have a virtual coffee together!</p> <p>Want to see if we're a match? Let's have a chat and find out. Schedule a free 30-minute strategy session to discuss your AI challenges and explore how we can work together.</p> <p>Book Intro Call </p> </li> </ul>"},{"location":"portfolio/projects/project_template/","title":"Portfolio Best Practices <p>This is a simplified example template. When creating your own portfolio:</p> <ul> <li>Include detailed technical challenges and how you solved them</li> <li>Add specific metrics and KPIs that demonstrate impact</li> <li>Show code snippets of interesting implementations</li> <li>Include architecture diagrams and system designs</li> <li>Document your decision-making process</li> <li>Highlight your specific contributions to the project</li> <li>Add visuals of the final product (if possible)</li> </ul>   <p>Case Study Summary</p> <p>Client:  Website:  Industry:   <p>Impact Metrics:</p> <ul> <li> <li> <li>&lt;PRIMARY KPI 3 (e.g., &lt;3s latency)&gt;</li> <li> <li>   <p>","text":""},{"location":"portfolio/projects/project_template/#challenge","title":"Challenge","text":"<p>&lt;2\u20133 sentences on the before-state: pain, constraints, and stakes. Mention data sources/silos, scale, and business urgency.&gt;</p>"},{"location":"portfolio/projects/project_template/#our-approach","title":"Our Approach","text":"<p>&lt;2\u20134 sentences summarizing the solution strategy. Name the core stages (e.g., ingestion \u2192 processing \u2192 modeling \u2192 serving \u2192 evaluation), key patterns (e.g., RAG, clustering, fine-tuning), and how it integrates with existing workflows.&gt;</p>"},{"location":"portfolio/projects/project_template/#results-impact","title":"Results &amp; Impact","text":"<ul> <li> <li> <li> <li> <li>"},{"location":"portfolio/projects/project_template/#solution-overview","title":"Solution Overview","text":"<p>Short caption describing the architecture or link to a higher-resolution diagram.</p>"},{"location":"portfolio/projects/project_template/#tech-stack","title":"Tech Stack","text":"<ul> <li> <li> <li> <li> <li> <li> <li> <li> <li>"},{"location":"portfolio/projects/project_template/#additional-context","title":"Additional Context","text":"<ul> <li>Timeline:  <li>Team Size:  <li>Role:  <li>Collaboration:  <li>Future Plans:  <ul> <li> <p> Let's have a virtual coffee together!</p> <p>Want to see if we're a match? Let's have a chat and find out. Schedule a free 30-minute strategy session to discuss your AI challenges and explore how we can work together.</p> <p>Book Intro Call </p> </li> </ul>"},{"location":"youtube/","title":"YouTube Channel","text":"<p>Redirecting to Miguel Miranda Dias YouTube Channel in a new window...</p> <p>If you're not redirected automatically, click here.</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/tools/","title":"Tools","text":""},{"location":"blog/category/python/","title":"Python","text":""},{"location":"blog/category/development/","title":"Development","text":""},{"location":"blog/category/data-science/","title":"Data Science","text":""}]}